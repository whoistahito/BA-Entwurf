
\chapter{State of the Art}
The field of automated job matching has undergone a substantial transformation, shifting from conventional keyword-based approaches to advanced semantic and  AI-driven systems. This evolution commenced with the adoption of semantic similarity approaches, which addressed numerous limitations of keyword-based matching. Research by Ajjam and Al-Raweshidy \cite{ajjam_ai-driven_2025} demonstrated that semantic models outperform keyword-based methods, achieving significantly higher similarity scores and enabling a more nuanced interpretation of the conceptual relevance between resumes and job descriptions.

This semantic understanding was further enhanced by the introduction of transformer models, particularly BERT-based architectures \cite{sentence-bert}. The development of domain-specific models represented a pivotal advancement in this field. For instance, Rosenberger et al. proposed CareerBERT, a novel approach that leverages the power of unstructured textual data sources, that uses jobGBERT \cite{gbert} to embed resumes and job categories derived from the standardized European Skills, Competences, and Occupations (ESCO) taxonomy \cite{careerbert}. By pre-training on specialized career data, such models create a shared embedding space that more accurately captures the intricacies of professional qualifications. Their approach demonstrated comparable, though more reliable, performance to that of GPT-4 while exhibiting significantly superior computational efficiency \cite{careerbert}.

As these technologies advanced, large-scale industrial platforms began to integrate them into their systems. CareerBuilder, for instance, developed an embedding-based recommender system that constructs fused embeddings from text, semantic entities, and location data to handle matching at a massive scale \cite{zhao2021embeddingbasedrecommender}. The two-stage process, involving approximate nearest neighbor search followed by a reranking model, led to significant gains in user engagement and match quality. LinkedIn's LinkSAGE framework represents a significant advancement in the field of network analysis, incorporating Graph Neural Networks (GNNs) to model the intricate and interwoven relationships within its professional network \cite{linksage}. By conceptualizing members, employment opportunities, and competencies as nodes in a comprehensive graph, LinkSAGE captures implicit signals and enhances relevance, underscoring the efficacy of graph-based methodologies in contemporary recommender systems. While this approach is highly effective within a closed ecosystem, its architecture presents significant barriers to broader adoption. The framework's dependency on a massive, proprietary database of user and job data makes it unsuitable for systems designed to aggregate vacancies from multiple platforms. Furthermore, the extensive infrastructure required to deploy and maintain such a large-scale graph network is prohibitively expensive for many applications. Despite these scalability and cost limitations, the success of LinkSAGE underscores a critical insight: the importance of modeling the detailed relationships between applicant skills and required job competencies, a principle that informs the methodology of this thesis.

Alongside these architectural advancements, recent research has focused on overcoming persistent data challenges, such as the sparsity of interaction labels in recruitment datasets. CONFIT V2 \cite{confitv2} presents a pioneering approach by leveraging LLMs to generate hypothetical reference resumes, thus enriching the training data. This technique is combined with a novel strategy called Runner-Up Hard-Negative Mining, which identifies highly similar but incorrect resume-job pairs to train the model. These examples teaches the model to discern subtle differences between suitable and unsuitable candidates, significantly improving performance and demonstrating the potential of generative AI to enhance contrastive learning frameworks.

Despite this progress, a research gap remains for a system that is both powerful and accessible. Industrial solutions such as LinkSAGE depend on substantial proprietary datasets and intricate infrastructure, while advanced academic models necessitate considerable computational resources for data augmentation. This thesis aims to address this gap by proposing a novel system that utilizes small open source LLMs for automated requirements extraction and token-embedding-driven similarity search, offering a scalable, transparent, and cost-effective solution.

\section{Related Work}
Following the general advancements in job matching systems, this section now examines prior research specifically focused on the two core techniques employed in this thesis: the use of LLMs for requirement extraction and the application of embedding models for similarity search.

\subsubsection{Requirements Extraction using LLMs}
LLMs have shown promising results in transforming unstructured job descriptions into structured formats. Howison et al. \cite{howison2024} demonstrated the use of generative AI to extract structured labor market data, such as education requirements and job types, from real-world job ads with statistically reliable results. Similarly, Herandi et al. \cite{herandiskilllm} proposed Skill-LLM, a fine-tuned LLM optimized for skill extraction, significantly outperforming standard Named Entity Recognition (NER) baselines in identifying hard skills. While Skill-LLM demonstrates the effectiveness of fine-tuned models for skill extraction, the approach of this thesis leverages general-purpose LLMs to avoid the need for task-specific fine-tuning, thus increasing accessibility. Nguyen et al. \cite{nguyen2024rethinkingskillextractionjob} explore few-shot prompt-based extraction using general-purpose LLMs, showing that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.

\subsubsection{Embedding and Similarity Search in Job Matching}
To enable dynamic and scalable job recommendations, recent systems increasingly embed user and job data into vector spaces and perform fast similarity searches. While domain-specific models like CareerBERT show reliable performance by leveraging standardized taxonomies \cite{careerbert}, Kurek et al. \cite{kurek2024} evaluated state of-the-art embedding models like sentence-transformers all-MiniLM-L6-v2 \cite{miniLm}, demonstrating strong retrieval performance in job matching without task-specific tuning. For tasks demanding higher accuracy, cross-encoder models are often superior. By processing the job and resume pair jointly, they can model the interaction between the two texts more deeply. As Siddharth notes in his work, ``Extensive comparative studies have demonstrated the superior performance of cross-encoder models compared to traditional ranking methods'' \cite{cross-encoder-metric}. This thesis therefore focuses on leveraging the high accuracy of a cross-encoder architecture for its core matching task.