\chapter{State of the Art}

The limitations of current job-matching systems, as outlined in the introduction, have led researchers to explore more sophisticated approaches capable of capturing the semantic relationships between job descriptions, requirements, and user profiles. Over the past years, the field has evolved from simple keyword-based methods toward advanced semantic, transformer-based, and large-scale industrial systems. This chapter provides an overview of these developments, focusing on those techniques that inform the design of the system proposed in this thesis.


\section{Semantic and Transformer-Based Job Matching}

Early improvements over keyword matching emerged through semantic similarity models, which aimed to interpret the conceptual relevance between resumes and job descriptions. Research by Ajjam and Al-Raweshidy demonstrated that these approaches outperform keyword-based methods, achieving significantly higher similarity scores and enabling a more nuanced interpretation of the conceptual relevance between resumes and job descriptions \cite{ajjam_ai-driven_2025}.

The adoption of transformer models marked a major shift in this domain. Architectures such as BERT and its derivatives \cite{sentence-bert} enabled richer contextual understanding and improved generalization across career‑related tasks. The development of domain-specific models represented a pivotal advancement in this field. For instance, Rosenberger et al. proposed CareerBERT, a novel approach that leverages the power of unstructured textual data sources, that uses jobGBERT \cite{gbert} to embed resumes and job categories derived from the standardized European Skills, Competences, and Occupations (ESCO) taxonomy \cite{careerbert}. By pre-training on specialized career data, such models create a shared embedding space that more accurately captures the intricacies of professional qualifications. Their approach demonstrated comparable, though more reliable, performance to that of GPT-4 while exhibiting significantly superior computational efficiency \cite{careerbert}.

These developments highlight the steady shift toward embedding-based architectures capable of encoding job content beyond surface-level keywords. However, most domain-specific transformer models rely heavily on curated data sources or custom pretraining pipelines, which limits accessibility.


\section{Industrial-Scale Job Matching Architectures} \label{sec:sota-linksage}

Parallel to academic research, large technology companies have integrated semantic embeddings into production‑level job matching systems. CareerBuilder, a company, that`` operates the largest job posting board in the U.S.'', developed an embedding-based job matching system that constructs fused embeddings from text, semantic entities, and location data to handle matching at a massive scale \cite{zhao2021embeddingbasedrecommender}. Their two-stage process, involving approximate nearest neighbor search followed by a reranking model, led to significant gains in user engagement and match quality \cite{zhao2021embeddingbasedrecommender}. \\
LinkedIn's LinkSAGE framework represents a significant advancement in the field of network analysis, incorporating Graph Neural Networks (GNN) to model the intricate and interwoven relationships within its professional network \cite{linksage}. LinkSAGE is using GraphSAGE architecture, which is ``a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data'' \cite{graphsage}. By conceptualizing members, employment opportunities, and competencies as nodes in a comprehensive graph, LinkSAGE is able to capture implicit signals and deliver highly relevant recommendations. Despite its effectiveness, this architecture poses significant barriers for broader adoption: it depends on massive proprietary datasets and requires extensive computational infrastructure, making it unsuitable for open, platform-aggregating systems such as the one developed in this thesis.
Nevertheless, LinkSAGE demonstrates a key insight for the field: effective job matching requires modeling the detailed relationships between applicant skills and job requirements. This principle directly informs the methodology of the approach proposed in this thesis.


\section{Addressing Data Challenges: Augmentation and Hard-Negative Mining} \label{sec:sota-confitv2}

As semantic models grew more advanced, researchers increasingly focused on addressing data sparsity and imbalance—persistent challenges in recruitment datasets. The CONFIT V2 framework introduces a novel strategy to enrich training data by using LLMs to generate reference resumes using Hypothetical Resume Embedding (HYRE) \cite{confitv2}. These synthetic examples are combined with Runner-Up Hard-Negative Mining, a technique that selects job–resume pairs which appear similar but are incorrect, forcing the model to learn subtle distinctions between suitable and unsuitable matches. This method significantly improves model robustness and highlights the growing role of LLMs in enhancing contrastive learning processes.

Although such methods push performance forward, they require substantial computational resources and specialized training pipelines. This dependency limits their applicability in lightweight, low-budget, or open-source contexts.


\section{Related Work}
Following the broader developments in job matching, this section examines research directly related to the two core techniques used in this thesis: (1) LLM‑based requirements extraction and (2) embedding-driven similarity search.

\subsection{Requirements Extraction using LLMs}
LLMs have demonstrated strong capabilities in transforming unstructured job descriptions into structured information. Howison et al. demonstrated the use of LLM to extract structured labor market data, such as education requirements and job types, from real-world job ads with statistically reliable results \cite{howison2024}. Similarly, Herandi et al. proposed Skill-LLM, a fine-tuned LLM optimized for skill extraction, significantly outperforming standard Named Entity Recognition (NER) baselines in identifying hard skills \cite{herandiskilllm}.
While fine-tuned models achieve strong performance, they require task-specific annotations and training. In contrast, methods such as those explored by Nguyen et al. demonstrate that general-purpose LLMs can achieve competitive results through few-shot prompting, particularly when dealing with syntactically complex skill mentions \cite{nguyen2024rethinkingskillextractionjob}. This reinforces the choice of this thesis to rely on general-purpose, open-source LLMs without any fine-tuning, thereby increasing accessibility and adaptability.

\subsection{Embedding and Similarity Search in Job Matching}
\label{introduction:similaritysearch}
Embedding-based similarity search has become a cornerstone of modern job recommendation systems. Domain-specific models such as CareerBERT achieve strong performance by leveraging structured taxonomies \cite{careerbert}. However, general-purpose models such as sentence-transformers (e.g., all-MiniLM-L6-v2) have also shown excellent retrieval performance without task‑specific training, as demonstrated by Kurek et al. \cite{kurek2024}. Building on this foundation, this thesis employs the model to process extracted requirements, enabling the system to capture semantic relationships effectively.

\section{Research Gap} \label{sota:research_gap}


The literature reviewed in this chapter demonstrates considerable progress in semantic job matching, domain‑specific transformer models, graph‑based architectures, and data‑augmentation techniques. Despite these advancements, several gaps persist:

\begin{itemize}
    \item \textbf{Open-Source:} Most high-performance matching architectures rely on proprietary architectures and datasets,limiting their transparency, reproducibility, and accessibility for further research.
    \item \textbf{Training Required:} State‑of‑the‑art approaches typically depend on extensive supervised training,domain-specific pre-training, or fine-tuning. These methods require large annotated datasets and significant computational resources, which limits their applicability in lightweight or low-resource contexts where such resources are often unavailable.
    \item \textbf{Utilization of LLMs:} Although recent studies demonstrate the effectiveness of LLMs for extracting structured requirements from unstructured job descriptions, existing job‑matching architectures rarely incorporate LLM‑based extraction as a central mechanism.


\end{itemize}

\subsection{LinkSAGE}
LinkSAGE is based on GNNs. The model architecture is defined as an ``encoder-decoder GNN model'', where the encoder utilizes the GraphSAGE algorithm to generate embeddings by aggregating neighbor information \cite{linksage}. Furthermore, model training is an essential component of LinkSAGE. This process is sequential: the system first employs ``inductive graph learning on a heterogeneous, evolving graph'' to train a GNN encoder, which is subsequently integrated into downstream ranking models via transfer learning \cite{linksage}. In terms of accessibility, LinkSAGE is a proprietary system and is not described as open source, as there is no link to a code repository for LinkSAGE provided in the paper.

\subsection{CONFIT V2}
As previously mentioned in \autoref{sec:sota-confitv2}, LLMs are an important component of CONFIT V2. It employs HYRE, a technique that uses an LLM ``to generate a hypothetical resume and augment the job post'' \cite{confitv2}. It also requires a distinct training phase centered on contrastive learning to optimize resume-job compatibility, where the encoder is trained using a specific loss function described as ``modified contrastive learning loss'' to distinguish between matching and non-matching pairs \cite{confitv2}. The training process is iterative, involving an initial training phase followed by a RUM step, as explained in \autoref{sec:sota-confitv2}. Regarding accessibility, the work is designated to be open source. As Yu et al. state, ``We will open-source our code and data (under license agreements) to provide a strong baseline for future research'' \cite{confitv2}.

\subsection{CareerBuilder}
The framework proposed by CareerBuilder relies on specific training protocols rather than pre-trained, zero-shot inference. First, it trains ``an end-to-end Deep Learning Embedding Model (DLEM) on a supervised learning task'' utilizing ``job application data'' \cite{zhao2021embeddingbasedrecommender}. The system also employs a representation learning model for the job-skill information graph, which uses ``Bayesian personalized ranking and margin-based loss functions to learn the vector representation''\cite{zhao2021embeddingbasedrecommender}. The architecture does not employ LLMs. The system is described as a proprietary in-house solution that serves as the ``core technology and service platform in CareerBuilder'' \cite{zhao2021embeddingbasedrecommender}, therfore it is not open source.

\subsection{Comparative Analysis of Job Matching Frameworks}
To highlight how this work distinguishes itself, \autoref{tab:model_comparison} summarizes the gaps explained above by comparing the discussed approaches against the proposed LRE and TSS.


\begin{table}[h]
    \caption{Systematic Comparison of Job Matching Approaches}
    \label{tab:model_comparison}
    \centering
    \begin{tabular*}{\textwidth}{| l@{\extracolsep\fill} r || c | c | c |}
        \hline

        Tools &
        \rotatebox{90}{Characteristics} &
        \rotatebox{90}{Uses LLMs} &
        \rotatebox{90}{Open-Source} &
        \rotatebox{90}{Training Required} &

        \hline
        \hline

        \multicolumn{2}{|l||}{LinkSAGE} &
        \xmark  &
        \xmark  &
        \cmark \\
        \hline

        \multicolumn{2}{|l||}{CareerBuilder} &
        \xmark &
        \xmark &
        \cmark \\
        \hline

        \multicolumn{2}{|l||}{CONFIT V2} &
        \cmark &
        \cmark &
        \cmark  \\
        \hline

        \hline
        \multicolumn{2}{|l||}{LRE + TSS} &
        \cmark &
        \cmark &
        \xmark \\
        \hline

    \end{tabular*}
\end{table}


In summary, although existing systems achieve strong performance in specific settings, there remains no approach that is semantically robust, deployable using lightweight, fully open‑source components. This thesis addresses this gap by proposing a job‑matching system that employs small open‑source LLMs for structured requirements extraction together with token-embedding‑based similarity search, thereby offering an accessible, and computationally efficient alternative.
