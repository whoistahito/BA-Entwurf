\chapter{Conclusion} \label{ch:conclusion}

This Chapter revisits research questions introduced in the \autoref{sec:research-questions} and summarizes the findings presented in the Evaluation chapter. Then \autoref{sec:discussion} reviews the design decisions and discusses the results. Finally \autoref{sec:outlook} provides an outlook for future works.

Regarding the first research question (RQ1), the evaluation confirmed that open-source LLMs can effectively extract structured job requirements from unstructured text without the need for task-specific fine-tuning. Integration of the outlines library successfully solved the challenge of syntactic validity, ensuring machine-readable output. However, the results indicate that the system is not yet at a production-ready level. While Qwen3-8B emerged as the best-performing model, it only achieved an aggregated overall score of 0.50 out of 1.0. Furthermore, critical semantic metrics such as Completeness and Alignment remained below 0.50 (0.45 and 0.50 respectively). This highlights a significant gap: while lightweight open-source models can be constrained to produce valid JSON structure, they still struggle with the semantic reasoning required to capture all mandatory requirements accurately without missing context or miscategorizing terms.

Regarding the second research question (RQ2), the comparison against LinkedIn’s recommendation engine showed that the proposed approach offers a more transparent and quantifiable matching logic. However, the system did not reach high similarity scores for the job postings. This low scoring can be linked to the weak requirement extraction process. As observed in the \autoref{sec:llm-performance}, the models frequently hallucinated requirements or included artifacts from the prompt instructions. These false negatives in the extracted data created a mismatch against the user profile, artificially lowering the similarity scores even for relevant job postings. Consequently, while the architecture provides a solid theoretical foundation, the current extraction quality introduces noise that lowers the reliability of the matching score.
Ultimately, this work contributes a modular, cost-effective architecture for job matching. By relying exclusively on lightweight open-source components, it democratizes access to advanced recruitment technologies, though significant improvements in extraction fidelity are required before it can compete with industrial-grade systems.

\section{Discussion} \label{sec:discussion}
The goal of this thesis was to present a novel, open-source approach for job matching that addresses the semantic limitations of traditional keyword-based search engines. By integrating LRE and TSS, the proposed system demonstrates a viable path toward more transparent and semantically grounded job matching.

\subsection{Lightweight Open-Source LLMs}
A central design decision was the integration of the outlines library to enforce valid JSON output.
As shown in the \autoref{sec:llm-performance}, this approach successfully solved the syntactic reliability issues often associated with LLMs. However, the evaluation highlights a critical distinction between syntactically valid and semantically correct. While the models were forced to produce the correct data structure, they frequently failed to populate it with accurate content. The decision to prioritize fully open‑source accessibility over the enhanced reasoning capabilities of proprietary frontier models introduced a performance trade‑off. While this choice ensured transparency, reproducibility, and ease of deployment on consumer‑grade hardware, it also constrained the system’s ability to reliably interpret and structure complex, unstandardized job descriptions. As a result, the overall architecture remains reliable but its effectiveness is currently limited by the underlying model's capacity to process complex, unstructured text.

\subsection{Assessment of the Evaluation Framework}
To assess the LLMs performance, the LLM-as-a-judge paradigm was employed using DeepEval Library. This methodological choice was driven by two key constraints. First, establishing a manual ground truth for the entire dataset would have been time-consuming and difficult to scale. Second, traditional evaluation methods based on exact keyword matching were considered insufficient, as they penalize valid extractions that are semantically equivalent but lexically different.
While the LLM-as-a-judge framework allowed for a scalable assessment of semantic alignment, the inclusion of the ``Readability'' metric proved to be methodologically redundant, since the inference pipeline mathematically guaranteed valid
JSON output as explained in \autoref{subsec:exception_handling}.

\subsection{Impact of Extraction Quality on Similarity Search}
The decision to employ the MaxSim algorithm with weighted field aggregation was intended to provide a granular semantic comparison. However, the performance of this module was heavily compromised by the GiGo principle, as explained in \autoref{subsec:evaluation-similarity-distribution}. The evaluation revealed that the similarity search yielded high false-negative rates, not because the embedding strategy was flawed, but because the extraction module failed to identify key skills. Consequently, the vector comparisons were often performed on empty or incomplete data sets. This demonstrates that maximizing the completeness of extracted requirements is more critical than maintaining strict precision.

\subsection{Prompting Strategies and Model Behavior}
The LRE relied on a zero-shot prompting strategy to maintain generality and avoid the need for task-specific examples. However, the resulting performance scores suggest that zero-shot inference may be too demanding for models in the 7B-9B parameter range, which often struggle to follow complex extraction rules without concrete examples to guide their output. Additionally, the evaluation uncovered a systematic ``copying bias'', as explained in \autoref{subsec:copying-bias}. While the precise cause of this behavior requires further investigation, it highlights a susceptibility of smaller models to overfit to the prompt instructions rather than the input data.

\section{Outlook} \label{sec:outlook}

While the proposed system demonstrates the efficacy of this approach, several limitations identified during implementation and evaluation point toward avenues for future research and optimization.

\subsection{Integration of Reasoning and ``Thinking'' Models}
The current implementation relied on standard instruction-tuned models (e.g.,mistral-7B-instruct, Llama-3.1-8B-Instruct). It did not utilize Chain-of-Thought (CoT) prompting or emerging ``thinking'' models that dedicate compute time to reasoning before generation. Future work should investigate if these architectures can improve the Completeness and Alignment scores by allowing the model to better distinguish between mandatory requirements and optional ``nice-to-haves''.
\subsection{Scaling to Larger Models}
The experiments were constrained to models with 7B–9B parameters to ensure consumer hardware compatibility. Given that extraction performance was the primary bottleneck, future research should evaluate larger open-source models (e.g., 70B+ parameters). These larger models typically exhibit superior instruction-following and semantic reasoning capabilities, which could significantly reduce the false positives and hallucinations observed in this study.
\subsection{Performance and Latency Analysis}
This thesis focused primarily on the accuracy and quality of extraction. The analysis of latency, to determine how fast LLM models extracted requirements, was out of scope of this work. For this system to be deployed as a real-time web service, a thorough performance benchmark is necessary, along with the exploration of optimization techniques such as quantization and batch processing.
\subsection{Data Engineering and Imbalance}
The data engineering process had its own limitations. The dataset contained more job postings from the technology industry than from all other industries combined. Future work should expand the dataset to include a balanced representation of industries, ensuring the system generalizes beyond tech-centric job descriptions.
\subsection{Explainability Features}
Finally, a key advantage of this structured approach is its potential for transparency. Future development should implement an explainability layer that translates the mathematical similarity scores into natural language. This feature would show the user exactly why a job is relevant (e.g., ``Matched based on your experience in Python and Project Management''), thereby bridging the gap between the numerical vector space and the user experience.