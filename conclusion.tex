\chapter{Conclusion}

\begin{enumerate}
    \item \textbf{Architecture over Size}: Despite being the oldest and smallest model in the benchmark, Mistral-7B (Overall Score: 0.46) outperformed the newer Llama-3.1-8B. This indicates that for specific information extraction tasks, model architecture and training data quality may be more decisive than raw parameter count or release date.

    \item \textbf{Readiness for Automation}: While Qwen3-8B achieved the top rank, the absolute scores (hovering around 0.5-0.6) and low pass rates (<30\% for strict correctness) indicate that 8B-parameter models are not yet fully reliable for fully autonomous, unsupervised extraction. They require either human-in-the-loop verification or further fine-tuning to reach production-grade reliability.
\end{enumerate}

\section{Outlook}
\begin{enumerate}
\item Thinking models are not considered, Data Processing was not perfect (data imbalance),
\item bigger models,
\item  Readability metric for evaluation was not so effective, because the for exception handling it retuned an empty object, so there were no output that had syntax errors.
    \item
    \item How fast the whole process is has not been measured.
\end{enumerate}