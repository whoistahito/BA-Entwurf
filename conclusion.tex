\chapter{Conclusion}

This thesis presented a novel, open-source approach to job matching that addresses the semantic limitations of traditional keyword-based search engines and the opacity of industrial recommendation systems. By integrating Large Language Model (LLM)-based requirements extraction with an embedding-driven similarity search, the proposed system demonstrates a viable path toward more transparent and semantically grounded job retrieval.

Regarding the first research question (RQ1), the evaluation confirmed that open-source LLMs can effectively extract structured job requirements from unstructured text without the need for task-specific fine-tuning. Integration of the outlines library successfully solved the challenge of syntactic validity, ensuring machine-readable output. However, the results indicate that the system is not yet at a production-ready level. While Qwen3-8B emerged as the best-performing model, it only achieved an aggregated overall score of 0.50 out of 1.0. Furthermore, critical semantic metrics such as Completeness and Alignment remained below 0.50 (0.45 and 0.50 respectively). This highlights a significant gap: while lightweight open-source models can be constrained to produce valid JSON structure, they still struggle with the semantic reasoning required to capture all mandatory requirements accurately without missing context or miscategorizing terms.

Regarding the second research question (RQ2), the comparison against LinkedIn’s recommendation engine showed that the proposed approach offers a more transparent and quantifiable matching logic. However, the system did not reach high similarity scores for the analyzed job postings. This low scoring can be linked to the weak requirement extraction process. As observed in the \autoref{sec:llm-performance}, the models frequently hallucinated requirements or included artifacts from the prompt instructions. These false negatives in the extracted data created a mismatch against the user profile, artificially lowering the similarity scores even for relevant job postings. Consequently, while the architecture provides a solid theoretical foundation, the current extraction quality introduces noise that lowers the reliability of the matching score.
Ultimately, this work contributes a modular, cost-effective architecture for job matching. By relying exclusively on lightweight open-source components, it democratizes access to advanced recruitment technologies, though significant improvements in extraction fidelity are required before it can compete with industrial-grade systems.

\section{Outlook}

While the proposed system demonstrates the efficacy of this approach, several limitations identified during implementation and evaluation point toward avenues for future research and optimization

\subsection{Integration of Reasoning and ``Thinking'' Models}
The current implementation relied on standard instruction-tuned models (e.g.,mistral-7B-instruct, Llama-3.1-8B-Instruct). It did not utilize Chain-of-Thought (CoT) prompting or emerging ``thinking'' models that dedicate compute time to reasoning before generation. Future work should investigate if these architectures can improve the Completeness and Alignment scores by allowing the model to better distinguish between mandatory requirements and optional ``nice-to-haves''.
\subsection{Scaling to Larger Models}
The experiments were constrained to models with 7B–9B parameters to ensure consumer hardware compatibility. Given that extraction performance was the primary bottleneck, future research should evaluate larger open-source models (e.g., 70B+ parameters). These larger models typically exhibit superior instruction-following and semantic reasoning capabilities, which could significantly reduce the false positives and hallucinations observed in this study.
