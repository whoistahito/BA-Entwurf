\chapter{Conclusion}

This thesis presented a novel, open-source approach to job matching that addresses the semantic limitations of traditional keyword-based search engines and the opacity of industrial recommendation systems. By integrating LLM-based requirements extraction with an embedding-driven similarity search, the proposed system demonstrates a viable path toward more transparent and semantically grounded job retrieval.

Regarding the first research question (RQ1), the evaluation confirmed that open-source LLMs can effectively extract structured job requirements from unstructured text without the need for task-specific fine-tuning. Integration of the outlines library successfully solved the challenge of syntactic validity, ensuring machine-readable output. However, the results indicate that the system is not yet at a production-ready level. While Qwen3-8B emerged as the best-performing model, it only achieved an aggregated overall score of 0.50 out of 1.0. Furthermore, critical semantic metrics such as Completeness and Alignment remained below 0.50 (0.45 and 0.50 respectively). This highlights a significant gap: while lightweight open-source models can be constrained to produce valid JSON structure, they still struggle with the semantic reasoning required to capture all mandatory requirements accurately without missing context or miscategorizing terms.

Regarding the second research question (RQ2), the comparison against LinkedIn’s recommendation engine showed that the proposed approach offers a more transparent and quantifiable matching logic. However, the system did not reach high similarity scores for the analyzed job postings. This low scoring can be linked to the weak requirement extraction process. As observed in the \autoref{sec:llm-performance}, the models frequently hallucinated requirements or included artifacts from the prompt instructions. These false negatives in the extracted data created a mismatch against the user profile, artificially lowering the similarity scores even for relevant job postings. Consequently, while the architecture provides a solid theoretical foundation, the current extraction quality introduces noise that lowers the reliability of the matching score.
Ultimately, this work contributes a modular, cost-effective architecture for job matching. By relying exclusively on lightweight open-source components, it democratizes access to advanced recruitment technologies, though significant improvements in extraction fidelity are required before it can compete with industrial-grade systems.

\section{Outlook}

While the proposed system demonstrates the efficacy of this approach, several limitations identified during implementation and evaluation point toward avenues for future research and optimization

\subsection{Integration of Reasoning and ``Thinking'' Models}
The current implementation relied on standard instruction-tuned models (e.g.,mistral-7B-instruct, Llama-3.1-8B-Instruct). It did not utilize Chain-of-Thought (CoT) prompting or emerging ``thinking'' models that dedicate compute time to reasoning before generation. Future work should investigate if these architectures can improve the Completeness and Alignment scores by allowing the model to better distinguish between mandatory requirements and optional ``nice-to-haves''.
\subsection{Scaling to Larger Models}
The experiments were constrained to models with 7B–9B parameters to ensure consumer hardware compatibility. Given that extraction performance was the primary bottleneck, future research should evaluate larger open-source models (e.g., 70B+ parameters). These larger models typically exhibit superior instruction-following and semantic reasoning capabilities, which could significantly reduce the false positives and hallucinations observed in this study.
\subsection{Performance and Latency Analysis}
This thesis focused primarily on the accuracy and quality of extraction. The analysis of latency, to determine how fast LLM models extracted requirements, was out of scope of this work. For this system to be deployed as a real-time web service, a thorough performance benchmark is necessary, along with the exploration of optimization techniques such as quantization and batch processing.
\subsection{Data Engineering and Imbalance}
The data engineering process had its own limitations . Dataset contained more job postings from the technology industry than from all other industries combined. Future work should expand the dataset to include a balanced representation of industries, ensuring the system generalizes beyond tech-centric job descriptions.
\subsection{Explainability Features}
Finally, a key advantage of this structured approach is its potential for transparency. Future development should implement an explainability layer that translates the mathematical similarity scores into natural language. This feature would show the user exactly why a job is relevant (e.g., ``Matched based on your experience in Python and Project Management''), thereby bridging the gap between the numerical vector space and the user experience.