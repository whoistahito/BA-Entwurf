\chapter{Conception}
Building on the identified research gap for an accessible yet powerful job matching framework, this chapter details the conception of the proposed filtering and matching system. It addresses the challenges and objectives outlined in the introduction. The system functions as a processing layer that builds upon the data acquisition capabilities of the YourJobFinder platform. The technical scope of this work begins after YourJobFinder has aggregated the raw, unstructured job descriptions based on an initial user query.

The architecture of the proposed system is illustrated in Figure \ref{fig:yourjobfinder_architecture}. The process is initiated when the web crawler provides raw HTML text to the backend. The work presented in this thesis focuses on the subsequent stages, which include a multi-stage pipeline designed to preprocess the data, extract structured requirements using a LLM, and perform a similarity search to identify relevant job postings. This chapter describes the two primary components of this framework: the requirements extraction process and the embedding-driven similarity search.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{images/DDD-Miro-7-11-bold}
    \caption{Diagram of the proposed system architecture}
    \label{fig:yourjobfinder_architecture}
\end{figure}

\section{Handling raw data}
The data pipeline begins with raw HTML job postings, as shown in Figure \ref{fig:yourjobfinder_architecture}. To eliminate irrelevant data like scripts and style tags and create a clean input for the LLM, the raw HTML is converted to Markdown using the markdownify library \cite{python-markdownify}. This pre-processing step ensures that the model focuses solely on the semantic content of the job description for requirements extraction.


\section{Schema based LLM Output}
A significant challenge in utilizing LLMs for information extraction is the non-deterministic nature of their output. To overcome this inconsistency and ensure all extracted data is machine-readable and standardized, a schema-guided generation approach is adopted. The outlines library \cite{willard2023outlines} is integrated into the system to force the LLM to generate responses that strictly adhere to a predefined JSON schema, which will be discussed further in the next chapter.  This method guarantees structural consistency across all processed job descriptions, which is crucial for the subsequent similarity search stage.

\section{LLM Models}
The models selected for the benchmark evaluation represent state-of-the-art open-source models as of October 2025:

\begin{table}[h]
	\caption{Model Parameter Sizes}
	\label{tab:model_params}
	\centering
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Model Name} & \textbf{Parameter Size} \\
		\hline
		\hline
		Qwen3 \cite{qwen3} & 8B  \\
		\hline
		GLM-4-0414 \cite{glm2024chatglm} & 9B \\
		\hline
		mistral-7B-instruct \cite{mistral-7b} & 7B  \\
		\hline
		Llama-3.1-8B-Instruct \cite{llama-3} & 8B  \\
		\hline
	\end{tabular}
\end{table}

\section{Managing Input Data}
This thesis employs a fixed-size chunking strategy. While semantic chunking represents a more sophisticated alternative designed to preserve contextual boundaries, recent research indicates that its significant computational overhead does not justify the potential performance gains \cite{semantic-chunking}. Therefore, fixed-size chunking was selected for its pragmatic balance of efficiency and effectiveness.

\subsection{Fixed-size chunking}
The number of tokens in a chunk and, optionally, whether there should be any overlap between them, is determined based on a fixed rule. In general, some overlap is desirable to prevent loss of semantic context between chunks. Fixed-size chunking is computationally efficient and simple to implement since it does not require any specialized techniques or libraries \cite{tamingllms}.


\section{LLM Output Evaluation}
A key challenge in evaluating the extracted requirements is the semantic variability inherent in LLM-generated text. A single requirement can be formulated in numerous, lexically distinct ways while remaining semantically correct. For instance, the requirement `Proven ability to manage projects simultaneously' could be correctly summarized as `Project management' or `Experience managing multiple projects'. Consequently, relying on a fixed, manually annotated ground truth becomes unreliable, since semantically equivalent requirements expressed with different wording would be incorrectly treated as mismatches.

To address this challenge, this thesis adopts an automated and dynamic evaluation strategy based on the 'LLM-as-a-judge' paradigm \cite{llm-as-a-judge}. Instead of relying on exact string matching, this approach uses a powerful LLM to assess the semantic faithfulness of the generated output against the source text. As Huyen mentions, ``AI judges are fast, easy to use, and relatively cheap compared to human evaluators. They can also work without reference data'' \cite{ai-engineering-book}. For this purpose, the DeepEval framework was employed \cite{deepeval}. DeepEval utilizes its G-Eval metric to prompt a judge model, which scores the quality of the extracted JSON object relative to the original job description \cite{G-eval}. The specific metrics guiding this evaluation will be discussed in the following section.

\subsection{LLM-as-a-Judge}
``The approach of using AI to evaluate AI is called AI as a judge or LLM as a judge'' \cite{ai-engineering-book}.
``An emerging method for evaluating generated content involves using other LLMs as evaluators'' \cite{blackbox-llmasjudge}. ``These evaluations can take various forms, including explanations, numeric values, or categorical rating'' \cite{blackbox-llmasjudge}. Using LLM-as-a-Judge is not just cost-effective, `` Studies have shown that certain AI judges are strongly correlated to human evaluators.'' \cite{ai-engineering-book}. Zheng et al. found in their work that LLM Judges and humans can achieve 85\% agreement, while the agreement among human evaluators was 81\% \cite{llm-as-a-judge}.


\subsection{G-Eval}
``G-EVAL is a prompt-based evaluator with three main components: 1) a prompt that contains the definition of the evaluation task and the desired evaluation criteria, 2) a chain-of-thoughts (CoT) that is a set of intermediate instructions generated by the LLM describing the detailed evaluation steps, and
3) a scoring function that calls LLM and calculates the score based on the probabilities of the return tokens.'' \cite{G-eval}

\subsection{Metrics} \label{metrics}
The LLM-as-a-judge evaluation was guided by four key metrics, adapted from the framework proposed by Dong Yuan et. al \cite{evaluation-mertric}. Each metric was specifically interpreted for the task of requirements extraction as follows:

\paragraph{Correctness} In the context of this thesis, this measure is used to evaluate whether the extracted job requirements are accurate and actually present in the input text.

\paragraph{Completeness} this metric is used to evaluate whether all mandatory job requirements from the input text were extracted.

\paragraph{Alignment} Applied to this task, alignment evaluates whether the model correctly avoided extracting `nice-to-have', `preferred', or `bonus' requirements.

\paragraph{Readability} Here, this metric is adapted to assess whether the extracted requirements are properly categorized as given into skills, experiences, and qualifications, and whether the output is well-structured, for instance, if it is free of syntax errors. This aspect will be explained deeper in the implementation section.


\section{Embedding and Similarity Search}
Subsequent to the extraction and structuring of job requirements,the system proceeds to the second stage, which addresses the objective of dynamic job matching. To accomplish this, the structured job data and user query are encoded into vector representations. The all-MiniLM-L6-v2 sentence transformer model was selected for this task due to its strong zero-shot retrieval performance in job matching as mentioned in \Ref{introduction:similaritysearch}. This process is designed to identify the top 10 most relevant job job postings based on semantic similarity, thereby providing a more accurate matching system than title-based filtering.
