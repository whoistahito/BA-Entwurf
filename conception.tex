\chapter{Conception}
TODO: rename chapters like Chapter~2
This chapter presents the conceptual design of the proposed job‑matching system, which addresses the limitations identified in the previous chapters. Building on the research gap outlined in \ref{sota:research_gap}, the goal of the conception is to translate the methodological requirements into a coherent system architecture that enables reliable requirements extraction and semantically informed similarity search. The system operates as a processing layer on top of the YourJobFinder platform, which serves as the data acquisition component by aggregating raw, unstructured job descriptions based on a user’s initial query.

Figure~\ref{fig:yourjobfinder_architecture} illustrates the overall workflow. The YourJobFinder crawler provides raw HTML content, after which the job matching system developed in this thesis is applied. This system consists of two central components:

\begin{itemize}
    \item a requirements extraction module based on open‑source LLMs,
and
    \item an embedding‑driven similarity search module that identifies semantically relevant job postings.
\end{itemize}

Together, these components form a lightweight, fully open‑source alternative to more resource‑intensive industrial systems, while still enabling meaningful semantic matching.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{images/DDD-Miro-7-11-bold}
    \caption{System architecture integrating YourJobFinder with the proposed job‑matching system.}
    \label{fig:yourjobfinder_architecture}
\end{figure}

\section{Handling raw data}
The pipeline begins with raw HTML job postings retrieved by the YourJobFinder crawler. In their original form, these documents often contain large amounts of non‑informative content, such as JavaScript, CSS, advertisements, and unrelated boilerplate text. To create a structured and semantically meaningful input for the LLM, the HTML is converted into Markdown using the \textit{markdownify} library \cite{python-markdownify}. This preprocessing step ensures that only the textual content relevant to the job description is retained, thereby reducing noise and improving extraction quality.


\section{Schema based LLM Output}
TODO: explanation about probabilistic function of outlines
A fundamental challenge when employing LLMs for requirements extraction is the inherent variability of their output. Even when provided with identical prompts, models may produce syntactically inconsistent or structurally divergent responses. To ensure that the extracted requirements are machine‑readable and uniform across all documents, this work employs a schema‑guided generation approach.

The \textit{outlines} library \cite{willard2023outlines} is integrated to enforce adherence to a predefined JSON schema, described in detail in \ref{lst:requirements_schema}. This guarantees that the LLM outputs a standardized structure containing the required fields (e.g., skills, qualifications, experience). Such structural consistency is essential, as it forms the
basis for downstream processing, evaluation, and similarity search.

\section{LLM Models}
The thesis evaluates several state‑of‑the‑art open‑source LLMs available as of October 2025. These models were selected based on their parameter size, availability, and suitability for constrained inference environments:
\begin{table}[h]
	\caption{Model Parameter Sizes}
	\label{tab:model_params}
	\centering
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Model Name} & \textbf{Parameter Size} \\
		\hline
		\hline
		Qwen3 \cite{qwen3} & 8B  \\
		\hline
		GLM-4-0414 \cite{glm2024chatglm} & 9B \\
		\hline
		mistral-7B-instruct \cite{mistral-7b} & 7B  \\
		\hline
		Llama-3.1-8B-Instruct \cite{llama-3} & 8B  \\
		\hline
	\end{tabular}
\end{table}

\section{Managing Input Data}
This thesis employs a fixed-size chunking strategy. While semantic chunking represents a more sophisticated alternative designed to preserve contextual boundaries, recent research indicates that its significant computational overhead does not justify the potential performance gains \cite{semantic-chunking}. Therefore, fixed-size chunking was selected for its pragmatic balance of efficiency and effectiveness.

\subsection{Fixed-size chunking}
The number of tokens in a chunk and, optionally, whether there should be any overlap between them, is determined based on a fixed rule. In general, some overlap is desirable to prevent loss of semantic context between chunks. Fixed-size chunking is computationally efficient and simple to implement since it does not require any specialized techniques or libraries \cite{tamingllms}.


\section{LLM Output Evaluation}
Evaluating the correctness of extracted requirements poses a unique methodological challenge, as semantically equivalent requirements may be expressed in lexically diverse forms. For example, `Proven ability to manage projects simultaneously' could be correctly summarized as `Project management' .even though the wording differs significantly.
Relying on exact string matching would thus penalize valid extractions.

To address this, the system employs an automated evaluation strategy based on the 'LLM-as-a-judge' paradigm \cite{llm-as-a-judge}. This approach uses an independent, more capable LLM to assess the faithfulness of extracted JSON fields relative to the original job
description. As Huyen mentions, ``AI judges are fast, easy to use, and relatively cheap compared to human evaluators. They can also work without reference data'' \cite{ai-engineering-book}. The evaluation framework used in this thesis is implemented with DeepEval \cite{deepeval}.

\subsection{LLM-as-a-Judge}
``The approach of using AI to evaluate AI is called AI as a judge or LLM as a judge'' \cite{ai-engineering-book}.
Evaluations may produce numeric scores or qualitative assessments \cite{blackbox-llmasjudge}. Using LLM-as-a-Judge is not just cost-effective, `` Studies have shown that certain AI judges are strongly correlated to human evaluators.'' \cite{ai-engineering-book}. Zheng et al. found in their work that LLM judges and humans can achieve 85\% agreement, while the agreement among human evaluators was 81\% \cite{llm-as-a-judge}.


\subsection{G-Eval}
The evaluation relies on the G‑Eval metric, a prompt‑based
 scoring method with three components: (1) task definition and criteria,
 (2) a chain‑of‑thought reasoning process used internally by the judge
model, and (3) a probabilistic scoring function \cite{G-eval}. This mechanism enables
controlled and interpretable evaluation of structured JSON outputs.

\subsection{Metrics} \label{metrics}
Four criteria adapted from Dong Yuan et al. \cite{evaluation-mertric}
guide the evaluation:


\begin{itemize}
    \item \textbf{Correctness} — assesses whether extracted requirements
 are accurate and present in the input text.
    \item \textbf{Completeness} — evaluates whether all mandatory
requirements have been captured.
    \item \textbf{Alignment} — measures whether the model avoids
including `preferred', `bonus', or non‑mandatory requirements.
    \item \textbf{Readability} — examines whether the output adheres to
the defined schema and is syntactically valid.
\end{itemize}
These metrics collectively ensure that extraction quality is judged not
only by semantic fidelity but also by structural reliability.

\section{Embedding and Similarity Search}
TODO: explain how embedding works
Once the job requirements have been extracted and validated, the second
component of the system performs semantic similarity matching. Both the
structured job postings and the user query are encoded as vector
representations using the \textit{all‑MiniLM‑L6‑v2} sentence transformer
 model. As discussed in Section~\ref{introduction:similaritysearch},
this model offers strong zero‑shot performance for retrieval‑based tasks
 in job matching.

The similarity search retrieves the ten most relevant job postings based
 on cosine similarity in the embedding space, thereby enabling a more
semantically precise ranking than simple title‑matching methods.

\subsection{Cross-Encoder}
``A cross-encoder allows two sentences to be passed to the Transformer
network simultaneously to predict the extent to which the two sentences are
similar. It does so by adding a classification head to the original architecture
that can output a similarity score'' \cite{handson-llms-2024}
Although more computationally expensive, this
architecture captures deeper contextual interactions and therefore
improves match quality.