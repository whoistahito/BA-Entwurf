\chapter{Conception}
This chapter presents the conceptual design of the proposed job‑matching system, which addresses the limitations identified in the previous chapters. Building on the research gap outlined in \ref{sota:research_gap}, the goal of the conception is to translate the methodological requirements into a coherent system architecture that enables reliable requirements extraction and semantically informed similarity search. The system operates as a processing layer on top of the YourJobFinder platform, which serves as the data acquisition component by aggregating raw, unstructured job descriptions based on a user’s initial query.

Figure~\ref{fig:yourjobfinder_architecture} illustrates the overall workflow. The YourJobFinder crawler provides raw HTML content, after which the job matching system developed in this thesis is applied. This system consists of two central components:

\begin{itemize}
    \item a requirements extraction module based on open‑source LLMs,
and
    \item an embedding‑driven similarity search module that identifies semantically relevant job postings.
\end{itemize}

Together, these components form a lightweight, fully open‑source alternative to more resource‑intensive industrial systems, while still enabling meaningful semantic matching.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{images/DDD-Miro-7-11-bold}
    \caption{System architecture integrating YourJobFinder with the proposed job‑matching system.}
    \label{fig:yourjobfinder_architecture}
\end{figure}

\section{Data Structure for Job Matching}

The core of the proposed system relies on a structured comparison between job requirements and user profiles. Figure~\ref{fig:data_structure} illustrates the fundamental data model that enables semantic matching. Both job postings and user profiles are decomposed into three key dimensions:

\begin{itemize}
    \item \textbf{Skills} — technical and professional competencies
    \item \textbf{Experience} — work history and domain expertise
    \item \textbf{Education or Certificates} — formal qualifications and certifications
\end{itemize}

These structured representations are encoded in a common JSON schema for both job descriptions and user profiles. The requirements extraction module converts unstructured job descriptions into this schema so that they share the same structure as user profiles. With this common schema, the system can compare skills, experience, and education in a consistent way, forming the basis for the LLM extraction step and the subsequent embedding-based similarity search.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{images/data-sctructure}
    \caption{Data structure mapping between job requirements and user profile for semantic matching.}
    \label{fig:data_structure}
\end{figure}

\section{Handling raw data}
The pipeline begins with raw HTML job postings retrieved by the YourJobFinder crawler. In their original form, these documents often contain large amounts of non‑informative content, such as JavaScript, CSS, advertisements, and unrelated boilerplate text. To create a structured and semantically meaningful input for the LLM, the HTML is converted into Markdown using the \textit{markdownify} library \cite{python-markdownify}. This preprocessing step ensures that only the textual content relevant to the job description is retained, thereby reducing noise and improving extraction quality.


\section{Schema based LLM Output}
TODO: explain outlines framework and how it works / forcing LLM to output structured using finite atomate \\
A fundamental challenge when employing LLMs for requirements extraction is the inherent variability of their output. Even when provided with identical prompts, models may produce syntactically inconsistent or structurally divergent responses. To ensure that the extracted requirements are machine‑readable and uniform across all documents, this work employs a schema‑guided generation approach.

The \textit{outlines} library \cite{willard2023outlines} is integrated to enforce adherence to a predefined JSON schema, described in detail in \ref{lst:requirements_schema}. This guarantees that the LLM outputs a standardized structure containing the required fields (e.g., skills, qualifications, experience). Such structural consistency is essential, as it forms the
basis for downstream processing, evaluation, and similarity search.

\section{LLM Models}
The thesis evaluates several state‑of‑the‑art open‑source LLMs available as of October 2025. These models were selected based on their parameter size, availability, and suitability for constrained inference environments:
\begin{table}[h]
	\caption{Model Parameter Sizes}
	\label{tab:model_params}
	\centering
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Model Name} & \textbf{Parameter Size} \\
		\hline
		\hline
		Qwen3 \cite{qwen3} & 8B  \\
		\hline
		GLM-4-0414 \cite{glm2024chatglm} & 9B \\
		\hline
		mistral-7B-instruct \cite{mistral-7b} & 7B  \\
		\hline
		Llama-3.1-8B-Instruct \cite{llama-3} & 8B  \\
		\hline
	\end{tabular}
\end{table}

\section{Managing Input Data}
This thesis employs a fixed-size chunking strategy. While semantic chunking represents a more sophisticated alternative designed to preserve contextual boundaries, recent research indicates that its significant computational overhead does not justify the potential performance gains \cite{semantic-chunking}. Therefore, fixed-size chunking was selected for its pragmatic balance of efficiency and effectiveness.

\subsection{Fixed-size chunking}
The number of tokens in a chunk and, optionally, whether there should be any overlap between them, is determined based on a fixed rule. In general, some overlap is desirable to prevent loss of semantic context between chunks. Fixed-size chunking is computationally efficient and simple to implement since it does not require any specialized techniques or libraries \cite{tamingllms}.


\section{LLM Output Evaluation}
Evaluating the correctness of extracted requirements poses a unique methodological challenge, as semantically equivalent requirements may be expressed in lexically diverse forms. For example, `Proven ability to manage projects simultaneously' could be correctly summarized as `Project management' .even though the wording differs significantly.
Relying on exact string matching would thus penalize valid extractions.

To address this, the system employs an automated evaluation strategy based on the 'LLM-as-a-judge' paradigm \cite{llm-as-a-judge}. This approach uses an independent, more capable LLM to assess the faithfulness of extracted JSON fields relative to the original job
description. As Huyen mentions, ``AI judges are fast, easy to use, and relatively cheap compared to human evaluators. They can also work without reference data'' \cite{ai-engineering-book}. The evaluation framework used in this thesis is implemented with DeepEval \cite{deepeval}.

\subsection{LLM-as-a-Judge}
``The approach of using AI to evaluate AI is called AI as a judge or LLM as a judge'' \cite{ai-engineering-book}.
Evaluations may produce numeric scores or qualitative assessments \cite{blackbox-llmasjudge}. Using LLM-as-a-Judge is not just cost-effective, `` Studies have shown that certain AI judges are strongly correlated to human evaluators.'' \cite{ai-engineering-book}. Zheng et al. found in their work that LLM judges and humans can achieve 85\% agreement, while the agreement among human evaluators was 81\% \cite{llm-as-a-judge}.


\subsection{G-Eval} \label{conception:g-eval}
The evaluation relies on the G‑Eval metric, a prompt‑based scoring method with three components: (1) task definition and criteria, (2) a chain‑of‑thought reasoning process used internally by the judge model, and (3) a probabilistic scoring function \cite{G-eval}. This mechanism enables controlled and interpretable evaluation of structured JSON outputs.

\subsection{Metrics} \label{metrics}
Four criteria adapted from Dong Yuan et al. \cite{evaluation-mertric} guide the evaluation:

\begin{itemize}
    \item \textbf{Correctness} — assesses whether extracted requirements are accurate and present in the input text.
    \item \textbf{Completeness} — evaluates whether all mandatory requirements have been captured.
    \item \textbf{Alignment} — measures whether the model avoids including `preferred', `bonus', or non‑mandatory requirements.
    \item \textbf{Readability} — examines whether the output adheres to the defined schema and is syntactically valid.
\end{itemize}
These metrics collectively ensure that extraction quality is judged not only by semantic fidelity but also by structural reliability.

\section{Embedding and Similarity Search}
Once the job requirements have been extracted and validated, the second component of the system performs semantic similarity matching. Both the structured job postings and the user query are encoded as vector
representations using the \textit{all‑MiniLM‑L6‑v2} sentence transformer model. As discussed in Section~\ref{introduction:similaritysearch},this model offers strong zero‑shot performance for retrieval‑based tasks in job matching.\\
The skills fields are weighted more because as studies show skills are more important in job application, for example a study by Bone et al. , that analysed online job vacanies from 2018 to 2024, points out ``We show that individual skills, rather than formal education requirements, have become increasingly important features of job advertisements for AI roles.'' \cite{skills-or-degree}.

Task of similarity search is to give a similarity score by embedding the extracted requirements and user profile, which have the same structure and comapring them using maxsim approach.

\subsection{Vector Embedding}
An embedding effectively maps discrete objects, like words or documents, to points in a continuous vector space \cite{llm-from-scratch}. Figure~\ref{fig:embeddings-process} illustrates this transformation from raw input to a vector representation, which serves as a good way to translate human language into computer language. \cite{handson-llms-2024}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/embeddings-process}
    \caption{the process of converting raw data into a three-dimensional numerical vector (Source: \cite{llm-from-scratch}).}
    \label{fig:embeddings-process}
\end{figure}

Embeddings are fixed in size, with their values representing various properties that collectively capture the meaning of a word in a way that is interpretable by a computer \cite{handson-llms-2024}. A key advantage of using embeddings is the ability to measure semantic similarity. By calculating distance metrics in the vector space, it is possible to determine how closely related two words are; words sharing similar meanings tend to be clustered closer together \cite{handson-llms-2024}, \autoref{fig:embeddings-distances} illustrates how words with similar meaning tend to be closer to each other, if it was possible to compress the embeddings into a two-dimensional representation \cite{handson-llms-2024}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/embeddings-distances}
    \caption{Embeddings of words that are similar will be close to each other in dimensional space (Source: \cite{handson-llms-2024}).}
    \label{fig:embeddings-distances}
\end{figure}

\subsection{MaxSim}

Computing a meaningful similarity score between the extracted job requirements and a user profile is not trivial. A naive pairwise comparison (for example, comparing the first skill in the job list with the first skill in the user profile) fails for several reasons:

\begin{itemize}
    \item the two lists often differ in length,
    \item items may appear in different orders,
    \item some elements may be repeated,
    \item and, crucially, missing elements in the user profile should
reduce the similarity score in a principled way.
\end{itemize}

To address these challenges, this thesis employs the MaxSim metric, an approach originally introduced in the context of machine translation evaluation \cite{maxsim}. MaxSim treats the comparison between two sets of embeddings as a maximum‑weight bipartite matching problem. Instead of aligning items by position, each element in the job requirement set is matched to the most semantically similar element in the user profile. This ensures that:

\begin{itemize}
    \item each requirement is paired with its best possible counterpart,
    \item semantically unrelated pairs do not artificially inflate the score,
    \item missing qualifications or skills in the user profile directly lower the overall similarity.
\end{itemize}

By selecting the maximum similarity for each requirement and aggregating these values, MaxSim produces a robust and interpretable similarity score. This makes it well suited for job‑matching scenarios, where the alignment between requirements and candidate attributes is inherently asymmetric and semantically nuanced.

Let $J = \{j_1, j_2, \dots, j_n\}$ be the set of extracted job requirements and $U = \{u_1, u_2, \dots, u_m\}$ be the set of items in the user profile. The algorithm proceeds in the following steps:

\begin{enumerate}
    \item \textbf{Embedding:} Both the job requirements and user attributes are mapped to a high-dimensional vector space using a sentence transformer model.
    \item \textbf{Similarity Matrix:} A similarity matrix $C$ is computed, where each entry $C_{ik}$ represents the cosine similarity between the $i$-th job requirement and the $k$-th user item.
    \item \textbf{Max-Over-Rows Aggregation:} For every specific job requirement $j_i$, the system identifies the maximum similarity score found across all user items $U$. This value represents the best evidence the user can provide for that specific requirement.
\end{enumerate}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/maxsim-matrix}
    \caption{Visualization of the similarity matrix. The system identifies strong semantic alignment (green cells) between user skills and job requirements, even when exact wording differs.}
    \label{fig:maxsim_matrix}
\end{figure}


The similarity score for a specific data field (e.g., Skills) is defined as the mean of these maximum satisfaction scores:

\begin{equation} \label{eq:maxsim_coverage}
    Score_{field} = \frac{1}{|J|} \sum_{i=1}^{|J|} \max_{k} (C_{ik})
\end{equation}


In the context of Figure \ref{fig:maxsim_matrix}, the calculation would proceed as follows:
\begin{itemize}
    \item Requirement \textit{Python}: Max match is 1.0 (from User: Python).
    \item Requirement \textit{Java}: Max match is 0.4 (from User: Python).
    \item Requirement \textit{Teamwork}: Max match is 0.8 (from User: Team player).
\end{itemize}
The final score for this section would be the average of these maximums: $(1.0 + 0.4 + 0.8) / 3 \approx 0.73$.


This approach ensures that every job requirement is evaluated against the user's best matching attribute, effectively measuring how much of the job description is ``covered'' by the user's profile.

\subsection{Weighted Field Aggregation}
Recognizing that not all data categories hold equal value in recruitment, the system segments the comparison into three dimensions: \textit{Skills}, \textit{Experiences}, and \textit{Qualifications}. The final semantic match score is a weighted linear combination of these field scores:

\begin{equation}
    Score_{total} = w_{skills} \cdot Score_{skills} + w_{exp} \cdot Score_{exp} + w_{qual} \cdot Score_{qual}
\end{equation}

By assigning higher weights to skills, the model aligns with recent findings on the increasing importance of skills-based hiring as explained above.
