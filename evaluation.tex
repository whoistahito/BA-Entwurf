\chapter{Evaluation}

This chapter presents the empirical evaluation of the system developed in this thesis and directly addresses the two research questions introduced in \autoref{ch:introduction}. The evaluation is structured into three components that build upon one another.

First, this chapter analyzes the characteristics of the dataset used for evaluation. This includes an examination of the distribution of job postings across sources, industries, and description lengths, providing the necessary context for understanding model behavior and extraction performance.

Secondly, evaluates how effectively open‑source Large Language Models extract structured job requirements from unstructured job descriptions (RQ1). This is assessed through a quantitative analysis using G‑Eval metrics within an automated ``LLM‑as‑a‑Judge’’ framework, complemented by a qualitative error analysis that identifies systematic strengths and weaknesses of the tested models.

Third, the chapter examines how the proposed job‑matching approach compares to conventional job search engines in terms of relevance and accuracy of retrieved postings (RQ2). This includes an evaluation of the similarity search pipeline and a comparison against traditional title‑based retrieval strategies.

Together, these three components provide a comprehensive empirical foundation for assessing the system’s performance, its practical utility, and its limitations, thereby preparing the ground for the discussion in the Outlook.


\section{Dataset}

The evaluation is based on a corpus of 209 job descriptions. Links to the postings were retrieved from the YourJobFinder database and subsequently scraped using the YourJobFinder's scraping tools. The raw HTML content of each posting was then converted into Markdown using the \textit{markdownify} library, ensuring that stylistic noise such as JavaScript, layout fragments, and boilerplate components were removed before the extraction phase.\\

\autoref{fig:jd_by_source} presents the distribution of job descriptions by source platform.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/job_descriptions_by_source}
    \caption{Job Descriptions categorized by source}
    \label{fig:jd_by_source}
\end{figure}


Approximately 95\% of all postings originate from large job portals, primarily Indeed and LinkedIn. The remaining postings stem from individual company career pages. \\

\autoref{fig:jd_by_industry} shows the sectoral distribution of the dataset. Roughly one third of all postings belong to the technology sector, with healthcare, business, and engineering also strongly represented. A small number of postings contain incomplete or minimal job descriptions, which provides an additional test for robustness in the extraction task. \\

\begin{figure}[hb!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/job_descriptions_by_industry}
    \caption{Job Descriptions categorized by industry}
    \label{fig:jd_by_industry}
\end{figure}

Beyond content distribution, the length of the job descriptions is an important characteristic for evaluating LLM behavior. \autoref{fig:jd_distribution_characters} illustrates the distribution of character lengths. \\

\begin{figure}[h!]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/distribution_job_description_lengths}
  \captionof{figure}{Distribution of job description lengths}
  \label{fig:jd_distribution_characters}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/character_count_by_source_violin}
  \captionof{figure}{Character count by source}
  \label{fig:jd_character_count_by_source}
\end{minipage}
    \label{fig:jd_character_comparasion}
\end{figure}


Job postings typically range between 6,000 and 8,000 characters, though posting length varies significantly across platforms. LinkedIn postings are generally the longest—often more than twice the length of Indeed postings—while Indeed descriptions have a median length of 6,053 characters. Notably, one posting lacking a structured description nevertheless contained more than 18,000 characters of general web content, offering an extreme but valuable case for testing extraction resilience.\\



\section{Evaluation Framework}

To systematically assess extraction quality as described in \autoref{conception:g-eval} using the \texttt{DeepEval} library, NVIDIA's hosted \texttt{gpt‑oss‑120b} model as an independent evaluator was employed.
Each output is rated along the four criteria introduced in \autoref{metrics}, then model outputs are averaged across the entire dataset to compute final performance scores.

\section{Performance Overview}

\autoref{fig:overall_model_ranking} presents the aggregated model ranking derived from the evaluation metrics. Qwen3-8B achieves the highest overall performance, followed by GLM-4-9B-0414 and Mistral-7B-Instruct-v0.3, with Llama-3.1-8B-Instruct showing the lowest aggregated scores.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/overall_model_ranking}
    \caption{Aggregated overall ranking across all evaluation metrics, rounded to two decimal places.}
    \label{fig:overall_model_ranking}
\end{figure}


To understand the drivers behind these overall scores, \autoref{tab:model_performance} details the breakdown across the specific G-Eval metrics. Each value represents a mean score across the evaluation corpus, where 1.0 signifies perfect compliance with the metric.

\begin{table}[h]
    \centering
    \caption{Model performance across G‑Eval metrics.}
    \label{tab:model_performance}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} &  \textbf{Correctness} & \textbf{Completeness} & \textbf{Alignment} & \textbf{Readability} \\
        \midrule
        \textbf{Qwen3-8B} &  \textbf{0.45} & 0.45 & \textbf{0.50} & \textbf{0.61} \\
        GLM-4-9B-0414            & 0.43 & \textbf{0.48} & 0.43 & 0.58 \\
        Mistral-7B-Instruct-v0.3         & 0.44 & 0.40 & 0.46 & 0.55 \\
        Llama‑3.1-8B-Instruct        & 0.38 & 0.37 & 0.39 & 0.49 \\
        \bottomrule
    \end{tabular}
    \vspace{0.2cm}

\small{\\ \textit{Scores represent mean values over all 209 evaluated job descriptions, rounded to two decimal places.}}
\end{table}

\subsection{Readability vs. Semantic Metrics}

An important observation from \autoref{tab:model_performance} is the disparity between Readability scores and the purely semantic metrics (Correctness and Completeness). Across all models, Readability scores (ranging from 0.49 to 0.61) are consistently higher than semantic-based scores. This performance can be linked to the specific architectural design and error-handling strategy in the extraction pipeline, which are
First, the use of the outlines library, which guarantees generating a valid JSON or deviate from the defined Pydantic schema. Second, the exception handling mechanism detailed in \autoref{subsec:exception_handling} that catches the exception during  inference.
As a result of these two design choices, the evaluation pipeline never encounters malformed or unstructured output. Every single output passed to the judge is, by definition, syntactically valid JSON.
However, despite these architectural guarantees of structural validity, the Readability scores do not reach the maximum value of 1.0. This gap highlights a critical distinction captured by the G-Eval metric, which evaluates not only whether the output is structured, but also whether requirements are ``properly categorized into skills, experiences, and qualifications''.
The failure to reach a perfect score indicates that while the models are forced to generate valid JSON, they still struggle with the semantic logic required to populate that structure correctly. The models frequently miscategorize requirements—for example, placing a formal degree under skills or a software tool under qualifications. Therefore, the Readability results lead to a clear conclusion: the problem of syntactic structure has been effectively solved through constraints (outlines and exception handling), leaving semantic reasoning and categorization as the primary bottleneck for open-source models.

\subsection{Precision vs. Completeness Trade-offs}
A clear trade‑off emerges between Qwen3‑8B and GLM-4-9B-0414.
Qwen3‑8B generally follows a conservative extraction pattern, rarely adding unsupported items and therefore achieving the highest precision. However, this sometimes results in missed implicit requirements, slightly lowering its completeness score.
In contrast, GLM-4-9B-0414 adopts a more liberal extraction strategy, capturing a broader range of potential requirements—including many optional or contextually weakly supported items—boosting completeness at the expense of precision.
For downstream filtering systems, conservative extraction is typically preferred to avoid incorrectly rejecting qualified candidates.

\subsection{Prompt Overfitting and Prompt Leakage}

A further issue observed across multiple models is \textit{prompt overfitting}. When job descriptions contained little to no usable content, both Qwen3‑8B and Llama-3.1-8B-Instruct repeatedly produced the same specific qualifications—most notably ``Bachelor’s degree in Computer Science’’ and ``PMP Certification.’’ These outputs closely resembled the few‑shot examples used in the system prompt.

This behavior aligns with the definition of \textit{prompt leakage}, which is defined as ``as the act of misaligning the original goal of a prompt to a new goal of printing part of or the whole original prompt instead'' \cite{prompt-leakage}.
Such leakage poses a systematic risk in extraction pipelines, as models may inadvertently encode example‑specific artifacts into their predictions. This finding underscores the need for prompt designs that minimize unintended signal strength, potentially by reducing example specificity or employing schema‑only zero‑shot prompting.

\section{Case Study}
To address RQ2 (\autoref{itm:rq2}), the proposed semantic matching pipeline was compared against LinkedIn's built-in job recommendation feature. LinkedIn serves as a practical baseline, as it suggests postings based on the information available in a user's profile (e.g., skills, experience, and inferred seniority). \\
A dedicated LinkedIn account was created to approximate a recent graduate seeking a full-time position after completing a bachelor's degree. The profile contained the following information:
\begin{itemize}
    \item \textbf{Skills:} Machine Learning, Domain-Driven Design (DDD), Data Science, Spring Boot, Java, Python, Hibernate, PostgreSQL, R, Project Management
    \item \textbf{Qualifications:} Domain Driven Design, B.Sc. Wirtschaftsinformatik
    \item \textbf{Experience:} 4 years of software development
\end{itemize}

LinkedIn's recommended jobs for this profile are illustrated in \autoref{fig:linkedin_screenshot}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Linkedin-screenshot}
    \caption{Example of LinkedIn job recommendations for the baseline profile.}
    \label{fig:linkedin_screenshot}
\end{figure}

\subsection{Comparison Procedure}
The comparison followed three steps:
\begin{enumerate}
    \item The first 23 job recommendations by LinkedIn were collected.
    \item Each job posting was manually marked as \textit{suitable} or \textit{not suitable} for the baseline profile, reflecting the preference of the hypothetical user.
    \item The same job descriptions were processed by the system developed in this thesis (requirements extraction + embedding-based similarity search), and similarity scores were computed against the user profile.
\end{enumerate}

\subsection{Similarity Score Distribution}
\autoref{fig:simlarity_score_distribtution} shows the bucketed similarity score distribution for the 23 LinkedIn-recommended jobs when using Qwen3-8B as the extraction model.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/bucketed_similarity_scores_qwen3-8b}
    \caption{Bucketed similarity score distribution for LinkedIn-recommended jobs (Qwen3-8B).}
    \label{fig:simlarity_score_distribtution}
\end{figure}

TODO: revisit the explanation, check left sekewed is wrong or right.
The distribution in \autoref{fig:simlarity_score_distribtution} is strongly left-skewed: 9 out of 23 postings (39.1\%) fall into the 0.0--0.1 bucket, while only 3 postings (13.0\%) achieve a score of \(\ge 0.4\). This indicates that, under the proposed semantic matching pipeline, the majority of LinkedIn recommendations constitute weak matches for the baseline profile, whereas only a small subset aligns strongly. However, some of the low similarity scores may also result from failures in the requirements extraction process, as discussed in \autoref{subsec:exception_handling}.


This behavior is consistent with the implemented scoring function: similarity is computed as a weighted MaxSim across extracted \textit{skills}, \textit{experiences}, and \textit{qualifications} (weights 0.5/0.3/0.2). For each field, the system computes cosine similarities between sentence-transformer embeddings (\texttt{all-MiniLM-L6-v2}), takes the maximum similarity for each job-side item, and averages these maxima; consequently, postings with many requirements that only partially overlap with the profile are penalized even if a few key skills match.

Finally, the 0.0--0.1 bucket may also include cases where requirements extraction fails: if the LLM output cannot be parsed into the schema, an empty Requirements object is returned, which yields a similarity score close to zero. Therefore, low scores in this bucket reflect a mixture of genuine mismatch and occasional extraction/formatting failures.

From the 23 LinkedIn-recommended jobs, the hypothetical user marked only 3 postings as preferred. Among these three, only a single posting obtained a comparatively high similarity score: \texttt{job21} with a score of 0.38.

This has direct implications for using a fixed acceptance threshold as a filtering mechanism. If a high threshold (e.g., 0.7) is applied, none of the user-preferred postings would pass, which is undesirable for a recommender/filtering system. Even when lowering the threshold to include \texttt{job21}, the remaining two user-preferred postings remain below the cutoff and therefore constitute false negatives.
