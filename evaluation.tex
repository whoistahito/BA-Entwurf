\chapter{Evaluation}

This chapter presents the empirical evaluation of the system developed in this thesis and directly addresses the two research questions introduced in \autoref{ch:introduction}. The evaluation is structured into three components that build upon one another.

First, this chapter analyzes the characteristics of the dataset used for evaluation. This includes an examination of the distribution of job postings across sources, industries, and description lengths, providing the necessary context for understanding model behavior and extraction performance.

Secondly, evaluates how effectively open‑source Large Language Models extract structured job requirements from unstructured job descriptions (RQ1). This is assessed through a quantitative analysis using G‑Eval metrics within an automated ``LLM‑as‑a‑Judge’’ framework, complemented by a qualitative error analysis that identifies systematic strengths and weaknesses of the tested models.

Third, the chapter examines how the proposed job‑matching approach compares to conventional job search engines in terms of relevance and accuracy of retrieved postings (RQ2). This includes an evaluation of the similarity search pipeline and a comparison against traditional title‑based retrieval strategies.

Together, these three components provide a comprehensive empirical foundation for assessing the system’s performance, its practical utility, and its limitations, thereby preparing the ground for the discussion in the Outlook.


\section{Dataset}

The empirical evaluation is based on a corpus of $N=209$ real-world job descriptions. To ensure the dataset reflects the practical challenges of a live job aggregator, links to the postings were retrieved directly from the \textit{YourJobFinder} database and scraped using the platform's existing scraping tools.

The raw HTML content of each posting was subsequently converted into Markdown .

\subsection{Source and Industry Distribution}

\autoref{fig:jd_by_source} illustrates the distribution of job postings by source platform. The dataset is heavily dominated by major job aggregators, with Indeed ($n=158$) and LinkedIn ($n=40$) accounting for approximately 95\% of the corpus. The remaining postings originate from niche platforms or individual company career pages.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/job_descriptions_by_source}
    \caption{Distribution of job postings by source platform}
    \label{fig:jd_by_source}
\end{figure}

In terms of sectoral distribution, \autoref{fig:jd_by_industry} shows that the dataset covers a diverse range of professional domains. While the technology sector is the largest single category (comprising roughly one-third of the dataset), there is significant representation from healthcare, business, and engineering. This diversity is essential for evaluating the model's generalization capabilities, ensuring that the extraction logic is not overfitted to the specific terminology or formatting conventions of the tech industry.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/job_descriptions_by_industry}
    \caption{Distribution of job postings by industry sector}
    \label{fig:jd_by_industry}
\end{figure}


\subsection{Length and Characteristics}

Beyond content distribution, the length of the job descriptions is a critical variable for evaluating LLM behavior, particularly regarding context window limitations. \autoref{fig:jd_distribution_characters} presents a histogram of character counts across the dataset.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/distribution_job_description_lengths}
    \caption{Histogram of job description lengths showing a right-skewed distribution}
    \label{fig:jd_distribution_characters}
\end{figure}

The distribution is strongly right-skewed, characterized by a significant disparity between the median length (6,003 characters) and the mean length (8,147 characters). While the majority of postings fall within the 4,000 to 8,000 character range, the dataset contains extreme outliers, with the longest single posting reaching 34,192 characters. This extreme variance—where the maximum length is nearly six times the median—validates the necessity of the hybrid chunking strategy introduced in Section 4.2.2, as these outliers would otherwise exceed the context window of standard 8B-parameter models.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/character_count_by_source_violin}
    \caption{Violin plot comparing character count distributions by source}
    \label{fig:jd_character_count_by_source}
\end{figure}

\autoref{fig:jd_character_count_by_source} further decomposes these statistics by source. A distinct structural difference is observable between platforms: Indeed postings exhibit a highly concentrated distribution with lower variance, suggesting a more standardized listing format. In contrast, LinkedIn postings display a much wider distribution with a significantly higher median length. This indicates that LinkedIn descriptions are not only more verbose but also less predictable in structure.
\clearpage

\section{Evaluation Framework}

To systematically assess extraction quality as described in \autoref{conception:g-eval} using the \texttt{DeepEval} library, NVIDIA's hosted \texttt{gpt‑oss‑120b} model as an independent evaluator was employed.
Each output is rated along the four criteria introduced in \autoref{metrics}, then model outputs are averaged across the entire dataset to compute final performance scores.

\section{Performance Overview}

\autoref{fig:overall_model_ranking} presents the aggregated model ranking derived from the evaluation metrics. Qwen3-8B achieves the highest overall performance, followed by GLM-4-9B-0414 and Mistral-7B-Instruct-v0.3, with Llama-3.1-8B-Instruct showing the lowest aggregated scores.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/overall_model_ranking}
    \caption{Aggregated overall ranking across all evaluation metrics, rounded to two decimal places.}
    \label{fig:overall_model_ranking}
\end{figure}


To understand the drivers behind these overall scores, \autoref{tab:model_performance} details the breakdown across the specific G-Eval metrics. Each value represents a mean score across the evaluation corpus, where 1.0 signifies perfect compliance with the metric.

\begin{table}[h]
    \centering
    \caption{Model performance across G‑Eval metrics.}
    \label{tab:model_performance}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} &  \textbf{Correctness} & \textbf{Completeness} & \textbf{Alignment} & \textbf{Readability} \\
        \midrule
        \textbf{Qwen3-8B} &  \textbf{0.45} & 0.45 & \textbf{0.50} & \textbf{0.61} \\
        GLM-4-9B-0414            & 0.43 & \textbf{0.48} & 0.43 & 0.58 \\
        Mistral-7B-Instruct-v0.3         & 0.44 & 0.40 & 0.46 & 0.55 \\
        Llama‑3.1-8B-Instruct        & 0.38 & 0.37 & 0.39 & 0.49 \\
        \bottomrule
    \end{tabular}
    \vspace{0.2cm}

\small{\\ \textit{Scores represent mean values over all 209 evaluated job descriptions, rounded to two decimal places.}}
\end{table}

\subsection{Readability vs. Semantic Metrics}

An important observation from \autoref{tab:model_performance} is the disparity between Readability scores and the purely semantic metrics (Correctness and Completeness). Across all models, Readability scores (ranging from 0.49 to 0.61) are consistently higher than semantic-based scores. This performance can be linked to the specific architectural design and error-handling strategy in the extraction pipeline, which are
First, the use of the outlines library, which guarantees generating a valid JSON or deviate from the defined Pydantic schema. Second, the exception handling mechanism detailed in \autoref{subsec:exception_handling} that catches the exception during  inference.
As a result of these two design choices, the evaluation pipeline never encounters malformed or unstructured output. Every single output passed to the judge is, by definition, syntactically valid JSON.
However, despite these architectural guarantees of structural validity, the Readability scores do not reach the maximum value of 1.0. This gap highlights a critical distinction captured by the G-Eval metric, which evaluates not only whether the output is structured, but also whether requirements are ``properly categorized into skills, experiences, and qualifications''.
The failure to reach a perfect score indicates that while the models are forced to generate valid JSON, they still struggle with the semantic logic required to populate that structure correctly. The models frequently miscategorize requirements—for example, placing a formal degree under skills or a software tool under qualifications. Therefore, the Readability results lead to a clear conclusion: the problem of syntactic structure has been effectively solved through constraints (outlines and exception handling), leaving semantic reasoning and categorization as the primary bottleneck for open-source models.

\subsection{Precision vs. Completeness Trade-offs}
A clear trade‑off emerges between Qwen3‑8B and GLM-4-9B-0414.
Qwen3‑8B generally follows a conservative extraction pattern, rarely adding unsupported items and therefore achieving the highest precision. However, this sometimes results in missed implicit requirements, slightly lowering its completeness score.
In contrast, GLM-4-9B-0414 adopts a more liberal extraction strategy, capturing a broader range of potential requirements—including many optional or contextually weakly supported items—boosting completeness at the expense of precision.
For downstream filtering systems, conservative extraction is typically preferred to avoid incorrectly rejecting qualified candidates.

\subsection{Prompt Overfitting and Prompt Leakage}

A further issue observed across multiple models is \textit{prompt overfitting}. When job descriptions contained little to no usable content, both Qwen3‑8B and Llama-3.1-8B-Instruct repeatedly produced the same specific qualifications—most notably ``Bachelor’s degree in Computer Science’’ and ``PMP Certification.’’ These outputs closely resembled the few‑shot examples used in the system prompt.

This behavior aligns with the definition of \textit{prompt leakage}, which is defined as ``as the act of misaligning the original goal of a prompt to a new goal of printing part of or the whole original prompt instead'' \cite{prompt-leakage}.
Such leakage poses a systematic risk in extraction pipelines, as models may inadvertently encode example‑specific artifacts into their predictions. This finding underscores the need for prompt designs that minimize unintended signal strength, potentially by reducing example specificity or employing schema‑only zero‑shot prompting.

\section{Case Study}
To address RQ2 (\autoref{itm:rq2}), the proposed semantic matching pipeline was compared against LinkedIn's built-in job recommendation feature. LinkedIn serves as a practical baseline, as it suggests postings based on the information available in a user's profile (e.g., skills, experience, and inferred seniority). \\
A dedicated LinkedIn account was created to approximate a recent graduate seeking a full-time position after completing a bachelor's degree. The profile contained the following information:
\begin{itemize}
    \item \textbf{Skills:} Machine Learning, Domain-Driven Design (DDD), Data Science, Spring Boot, Java, Python, Hibernate, PostgreSQL, R, Project Management
    \item \textbf{Qualifications:} Domain Driven Design, B.Sc. Wirtschaftsinformatik
    \item \textbf{Experience:} 4 years of software development
\end{itemize}

LinkedIn's recommended jobs for this profile are illustrated in \autoref{fig:linkedin_screenshot}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Linkedin-screenshot}
    \caption{Example of LinkedIn job recommendations for the baseline profile.}
    \label{fig:linkedin_screenshot}
\end{figure}

\subsection{Comparison Procedure}
The comparison followed three steps:
\begin{enumerate}
    \item The first 23 job recommendations by LinkedIn were collected.
    \item Each job posting was manually marked as \textit{suitable} or \textit{not suitable} for the baseline profile, reflecting the preference of the hypothetical user.
    \item The same job descriptions were processed by the system developed in this thesis (requirements extraction + embedding-based similarity search), and similarity scores were computed against the user profile.
\end{enumerate}

\subsection{Similarity Score Distribution}
\autoref{fig:simlarity_score_distribtution} shows the bucketed similarity score distribution for the 23 LinkedIn-recommended jobs when using Qwen3-8B as the extraction model.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/bucketed_similarity_scores_qwen3-8b}
    \caption{Bucketed similarity score distribution for LinkedIn-recommended jobs (Qwen3-8B).}
    \label{fig:simlarity_score_distribtution}
\end{figure}

The distribution in \autoref{fig:simlarity_score_distribtution} is strongly right-skewed: 9 out of 23 postings (39.1\%) fall into the 0.0--0.1 bucket, while only 3 postings (13.0\%) achieve a score of \(\ge 0.4\). This indicates that, under the proposed semantic matching pipeline, the majority of LinkedIn recommendations constitute weak matches for the baseline profile, whereas only a small subset aligns strongly. However, some of the low similarity scores may also result from failures in the requirements extraction process, as discussed in \autoref{subsec:exception_handling}.

Finally, the 0.0--0.1 bucket may also include cases where requirements extraction fails: if the LLM output cannot be parsed into the schema, an empty Requirements object is returned, which yields a similarity score close to zero. Therefore, low scores in this bucket reflect a mixture of genuine mismatch and occasional extraction/formatting failures.

From the 23 LinkedIn-recommended jobs, the hypothetical user marked only 3 postings as preferred. Among these three, only a single posting obtained a comparatively high similarity score: \texttt{job21} with a score of 0.38.

This has direct implications for using a fixed acceptance threshold as a filtering mechanism. If a high threshold (e.g., 0.7) is applied, none of the user-preferred postings would pass, which is undesirable for a recommender/filtering system. Even when lowering the threshold to include \texttt{job21}, the remaining two user-preferred postings remain below the cutoff and therefore constitute false negatives.
