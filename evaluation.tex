\chapter{Evaluation}

This chapter presents the empirical evaluation of the system developed in this thesis and directly addresses the two research questions introduced in \autoref{ch:introduction}. The evaluation is structured into three components that build upon one another.

First, this chapter analyzes the characteristics of the dataset used for evaluation. This includes an examination of the distribution of job postings across sources, industries, and description lengths, providing the necessary context for understanding model behavior and extraction performance.

Secondly, evaluates how effectively open‑source Large Language Models extract structured job requirements from unstructured job descriptions (RQ1). This is assessed through a quantitative analysis using G‑Eval metrics within an automated ``LLM‑as‑a‑Judge’’ framework,complemented by a qualitative error analysis that identifies systematic strengths and weaknesses of the tested models.

Third, the chapter examines how the proposed job‑matching approach compares to conventional job search engines in terms of relevance and accuracy of retrieved postings (RQ2). This includes an evaluation of the similarity search pipeline and a comparison against traditional title‑based retrieval strategies.

Together, these three components provide a comprehensive empirical foundation for assessing the system’s performance, its practical utility, and its limitations, thereby preparing the ground for the discussion in the Outlook.


\section{Dataset}

The evaluation is based on a corpus of 209 job descriptions. Links to the postings were retrieved from the YourJobFinder database and subsequently scraped using the YourJobFinder's scraping tools. The raw HTML content of each posting was then converted into Markdown using the \textit{markdownify} library, ensuring that stylistic noise such as JavaScript, layout fragments, and boilerplate components were removed before the extraction phase.\\

Figure~\ref{fig:jd_by_source} presents the distribution of job descriptions by source platform.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/job_descriptions_by_source}
    \caption{Job Descriptions categorized by source}
    \label{fig:jd_by_source}
\end{figure}


Approximately 95\% of all postings originate from large job portals, primarily Indeed and LinkedIn. The remaining postings stem from individual company career pages. Figure~\ref{fig:jd_by_industry} shows the sectoral distribution of the dataset.


\begin{figure}[hb!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/job_descriptions_by_industry}
    \caption{Job Descriptions categorized by industry}
    \label{fig:jd_by_industry}
\end{figure}

Roughly one third of all postings belong to the technology sector, with healthcare, business, and engineering also strongly represented. A small number of postings contain incomplete or minimal job descriptions, which provides an additional test for robustness in the extraction task. \\

Beyond content distribution, the length of the job descriptions is an important characteristic for evaluating LLM behavior. Figure~\ref{fig:jd_character_comparasion} illustrates the distribution of character lengths. \\

\begin{figure}[h!]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/distribution_job_description_lengths}
  \captionof{figure}{Distribution of job description lengths}
  \label{fig:jd_distribution_characters}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/character_count_by_source_violin}
  \captionof{figure}{Character count by source}
  \label{fig:jd_character_count_by_source}
\end{minipage}
    \label{fig:jd_character_comparasion}
\end{figure}


Job postings typically range between 6,000 and 8,000 characters, though posting length varies significantly across platforms. LinkedIn postings are generally the longest—often more than twice the length of Indeed postings—while Indeed descriptions have a median length of 6,053 characters. Notably, one posting lacking a structured description nevertheless contained more than 18,000 characters of general web content, offering an extreme but valuable case for testing extraction resilience.\\



\section{Evaluation Framework}

To systematically assess extraction quality, an automated evaluation pipeline based on the ``LLM‑as‑a‑Judge'' paradigm was implemented using the \texttt{DeepEval} library. This method employs NVIDIA's hosted \texttt{gpt‑oss‑120b} model as an independent evaluator, ensuring consistency and reducing the cost and subjectivity associated with human annotation.

Evaluation is performed using the G‑Eval framework, which defines a structured scoring process including (i) task‑specific evaluation criteria, (ii) internal chain‑of‑thought reasoning, and (iii) a probability‑based scoring mechanism \cite{G-eval}. Each output is rated along the four criteria introduced in Section~\ref{metrics}:
\begin{itemize}
    \item Correctness
    \item Completeness
    \item Alignment
    \item Readability
\end{itemize}

Model outputs are averaged across the entire dataset to compute final performance scores.

\section{Model Performance Comparison}

Figure~\ref{fig:overall_model_ranking} presents the aggregated overall model ranking derived from the evaluation metrics.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/overall_model_ranking}
    \caption{Aggregated overall ranking across all evaluation metrics.}
    \label{fig:overall_model_ranking}
\end{figure}

Qwen3‑8B achieves the highest overall performance, followed by GLM‑4‑9B, Mistral‑7B‑Instruct, and finally Llama‑3.1‑8B‑Instruct.

\section{Quantitative Performance Analysis}

Table~\ref{tab:model_performance} summarizes the quantitative results across the five G‑Eval metrics. Each value represents a mean score across the evaluation corpus, where 1.0 signifies perfect compliance with the metric.

\begin{table}[h]
    \centering
    \caption{Model performance across G‑Eval metrics.}
    \label{tab:model_performance}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{Overall} & \textbf{Structure} & \textbf{Completeness} & \textbf{Correctness} & \textbf{Precision} \\
        \midrule
        \textbf{Qwen 3 8B} & \textbf{0.50} & \textbf{0.61} & 0.45 & \textbf{0.45} & \textbf{0.50} \\
        GLM 4 9B           & 0.48 & 0.58 & \textbf{0.48} & 0.43 & 0.43 \\
        Mistral 7B         & 0.46 & 0.55 & 0.40 & 0.44 & 0.46 \\
        Llama 3.1 8B       & 0.41 & 0.49 & 0.37 & 0.38 & 0.39 \\
        \bottomrule
    \end{tabular}
    \vspace{0.2cm}
    \small{\\ \textit{Scores represent mean values over all 209 evaluated job descriptions.}}
\end{table}

Qwen3‑8B consistently yields the highest scores in structure, correctness, and precision. GLM‑4‑9B achieves the strongest completeness score, while Mistral‑7B offers more balanced but lower overall performance. Llama‑3.1‑8B underperforms across all metrics, primarily due to hallucinated requirements and structural inconsistencies.


\section{Qualitative Error Analysis}

Beyond numerical results, a qualitative examination reveals distinctive behavioral patterns and recurrent error types that explain the relative ranking of the models.

\subsection{Hallucination of Generic Requirements}

One of the most notable issues, particularly for the Llama‑3.1‑8B model, is the insertion of highly stereotypical requirements that do not appear in the original text. Examples include fabricating educational requirements such as a ``Bachelor’s degree in Computer Science’’ or professional qualifications like ``PMP Certification.’’ These hallucinations suggest that such models rely too strongly on prior knowledge about typical job postings rather than grounding their extraction strictly in the input content. This poses a critical challenge for fairness in applicant filtering, as it may introduce exclusionary criteria not stated by employers.

\subsection{Precision–Recall Trade-Off Between Models}

A clear trade‑off emerges between Qwen3‑8B and GLM‑4‑9B.
Qwen3‑8B generally follows a conservative extraction pattern, rarely adding unsupported items and therefore achieving the highest precision. However, this sometimes results in missed implicit requirements, slightly lowering its completeness score.
In contrast, GLM‑4‑9B adopts a more liberal extraction strategy, capturing a broader range of potential requirements—including many optional or contextually weakly supported items—boosting completeness at the expense of precision.
For downstream filtering systems, conservative extraction is typically preferred to avoid incorrectly rejecting qualified candidates.

\subsection{Prompt Overfitting and Prompt Leakage}

A further issue observed across multiple models is \textit{prompt overfitting}. When job descriptions contained little to no usable content, both Qwen3‑8B and Llama‑3.1‑8B repeatedly produced the same specific qualifications—most notably ``Bachelor’s degree in Computer Science’’ and ``PMP Certification.’’ These outputs closely resembled the few‑shot examples used in the system prompt.

This behavior aligns with the definition of \textit{prompt leakage},which is defined as ``as the act of misaligning the original goal of a prompt to a new goal of printing part of or the whole original prompt instead'' \cite{prompt-leakage}.
Such leakage poses a systematic risk in extraction pipelines, as models may inadvertently encode example‑specific artifacts into their predictions. This finding underscores the need for prompt designs that minimize unintended signal strength, potentially by reducing example specificity or employing schema‑only zero‑shot prompting.

\section{Case Study}
To address RQ2 (\autoref{itm:rq2}), the proposed semantic matching pipeline was compared against LinkedIn's built-in job recommendation feature. LinkedIn serves as a practical baseline, as it suggests postings based on the information available in a user's profile (e.g., skills, experience, and inferred seniority). \\
A dedicated LinkedIn account was created to approximate a recent graduate seeking a full-time position after completing a bachelor's degree. The profile contained the following information:
\begin{itemize}
    \item \textbf{Skills:} Machine Learning, Domain-Driven Design (DDD), Data Science, Spring Boot, Java, Python, Hibernate, PostgreSQL, R, Project Management
    \item \textbf{Qualifications:} Domain Driven Design, B.Sc. Wirtschaftsinformatik
    \item \textbf{Experience:} 4 years of software development
\end{itemize}

LinkedIn's recommended jobs for this profile are illustrated in \autoref{fig:linkedin_screenshot}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Linkedin-screenshot}
    \caption{Example of LinkedIn job recommendations for the baseline profile.}
    \label{fig:linkedin_screenshot}
\end{figure}

\subsection{Comparison Procedure}
The comparison followed three steps:
\begin{enumerate}
    \item The first 23 job recommendations by LinkedIn were collected.
    \item Each job posting was manually marked as \textit{suitable} or \textit{not suitable} for the baseline profile, reflecting the preference of the hypothetical user.
    \item The same job descriptions were processed by the system developed in this thesis (requirements extraction + embedding-based similarity search), and similarity scores were computed against the user profile.
\end{enumerate}

\subsection{Similarity Score Distribution}
\autoref{fig:simlarity_score_distribtution} shows the bucketed similarity score distribution for the 23 LinkedIn-recommended jobs when using Qwen3-8B as the extraction model.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/bucketed_similarity_scores_qwen3-8b}
    \caption{Bucketed similarity score distribution for LinkedIn-recommended jobs (Qwen3-8B).}
    \label{fig:simlarity_score_distribtution}
\end{figure}

The distribution in \autoref{fig:simlarity_score_distribtution} is strongly left-skewed: 9 out of 23 postings (39.1\%) fall into the 0.0--0.1 bucket, while only 3 postings (13.0\%) achieve a score of \(\ge 0.4\). This indicates that, under the proposed semantic matching pipeline, the majority of LinkedIn recommendations constitute weak matches for the baseline profile, whereas only a small subset aligns strongly.

This behavior is consistent with the implemented scoring function: similarity is computed as a weighted MaxSim across extracted \textit{skills}, \textit{experiences}, and \textit{qualifications} (weights 0.5/0.3/0.2). For each field, the system computes cosine similarities between sentence-transformer embeddings (\texttt{all-MiniLM-L6-v2}), takes the maximum similarity for each job-side item, and averages these maxima; consequently, postings with many requirements that only partially overlap with the profile are penalized even if a few key skills match.

Finally, the 0.0--0.1 bucket may also include cases where requirements extraction fails: if the LLM output cannot be parsed into the schema, an empty Requirements object is returned, which yields a similarity score close to zero. Therefore, low scores in this bucket reflect a mixture of genuine mismatch and occasional extraction/formatting failures.

From the 23 LinkedIn-recommended jobs, the hypothetical user marked only 3 postings as preferred. Among these three, only a single posting obtained a comparatively high similarity score: \texttt{job21} with a score of 0.38.

This has direct implications for using a fixed acceptance threshold as a filtering mechanism. If a high threshold (e.g., 0.7) is applied, none of the user-preferred postings would pass, which is undesirable for a recommender/filtering system. Even when lowering the threshold to include \texttt{job21}, the remaining two user-preferred postings remain below the cutoff and therefore constitute false negatives.
