\chapter{Evaluation}
This chapter presents a quantitative evaluation of the selected open-source Large Language Models to determine their effectiveness in extracting structured job requirements from unstructured text. This analysis directly addresses the second research question (RQ2) by assessing which model demonstrates the best performance in terms of correctness, completeness, readability, and alignment. The performance of each model is measured across three dimensions: (i) an overall model ranking derived from aggregated scores, (ii) a detailed comparison of average scores for each of the four evaluation metrics, and (iii) an analysis of task completion pass rates. The presented results provide the foundation for the subsequent summary of findings and the discussion of future research directions in the Outlook.


\section{Model Performance Comparison}
In this section, we evaluate the performance of the different models based on three key metrics: overall model ranking, passed rates, and average scores. The following figures illustrate the comparative performance of the models in each of these areas.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/overall_model_ranking}
    \caption{Overall model ranking}
    \label{fig:overall_model_ranking}
\end{figure}

Figure \ref{fig:overall_model_ranking} presents the overall ranking of the models. This ranking is an overall score derived from multiple metrics as discussed before see \Ref{metrics}. As can be seen, Qwen3-8B achieves the highest rank, followed by GLM-4. This suggests that, in aggregate, Qwen3-8B demonstrates superior performance across the evaluated criteria.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/average_scores_comparison}
    \caption{Overall average performance score of the evaluated LLM models across all four metrics}
    \label{fig:average_scores_comparison}
\end{figure}

Figure \ref{fig:average_scores_comparison} shows comparison of  the average scores obtained by each model in all metrics. The score is a measure of the quality of the model's output, with higher scores indicating better performance. The results in this figure are consistent with the previous metrics, showing Model Qwen3-8B with the highest average score. This further supports the conclusion that Model Qwen3-8B is the most effective model in this evaluation. It is important to note that Qwen3-8B has better percentage in Alignment category than GLM-4 but lower percentage in Completeness, which means that Qwen3-8B identifies 'must-have' requirements better but hallucinates more compared to GLM-4.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/passed_rates_comparison}
    \caption{Passed rates comparison}
    \label{fig:passed_rates_comparison}
\end{figure}

Figure \ref{fig:passed_rates_comparison} shows a comparison of the passed rates for each model in all metrics. G-Eval has the option to define a success threshold, which was set to 0.7 for all metrics. Based on that the passed rate is defined as the percentage of test cases that the model successfully completes. The chart indicates that Model Qwen3-8B has the highest passed rate, reinforcing its strong performance. Models GLM-4 and mistral-7B-instruct have nearly the same passed rate



\section{Summary of Evaluation}

\section{Outlook}
