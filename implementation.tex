\chapter{Implementation}
\label{ch:implementation}

This chapter details the technical realization of the concept presented in the previous chapter. It describes the development environment, the data processing pipeline, the implementation of the requirements extraction using LLMs, the evaluation methodology, and the realization of the similarity search algorithms.

\section{Development Environment}
The system was implemented using the Python programming language (Version 3.10), chosen for its extensive ecosystem of data science and machine learning libraries. The core dependencies include:
\begin{itemize}
	\item \textbf{Markdownify}: For converting raw HTML content into structured Markdown text.
	\item \textbf{Outlines}: To enforce structured JSON generation from the LLMs, ensuring deterministic output formats.
	\item \textbf{FastAPI}: For serving the inference engine as a scalable web service.
	\item \textbf{Sentence-Transformers}: For generating vector embeddings of job descriptions and user queries using Bi-Encoders and Cross-Encoders.
	\item \textbf{DeepEval}: For the automated ``LLM-as-a-judge'' evaluation pipeline.
\end{itemize}
All experiments and inference tasks were conducted on a dual GPU setup (2x 12GB NVIDIA RTX 3080), ensuring consistent performance benchmarks across all tested models. Figure \ref{fig:system_architecture} illustrates the high-level architecture of the implemented system.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth,height=0.9\textwidth]{images/system-architecture2.pdf}
	\caption{System Architecture}
	\label{fig:system_architecture}
\end{figure}


\section{Data Preprocessing}
As outlined in the conception, the raw data consists of HTML documents. The preprocessing pipeline transforms this unstructured input into clean, chunked text suitable for LLM processing.

\subsection{HTML to Markdown Conversion}
The \texttt{markdownify} library is configured to strip unnecessary tags (such as scripts, styles, and images) while preserving the structural hierarchy of the document. Headings, lists, and paragraphs are retained, as these structural elements provide critical semantic cues for the extraction model. This reduction significantly lowers the token count while maintaining the semantic structure of the job posting.

\subsection{Hybrid Chunking Strategy}
To process job descriptions that exceed the context window of smaller LLMs, a hybrid chunking strategy is implemented. The text is primarily split based on Markdown headers (using the regex \texttt{r'(\#\{1,6\}\textbackslash s+.*? \textbackslash n)'}). This ensures that logical sections—such as 'Responsibilities' or 'Requirements'—remain intact within a single context window.

A fallback mechanism is implemented for cases where a section exceeds the maximum token limit. If a semantic chunk exceeds the configured size (defaulting to approximately 12,000 tokens, estimated via character count), it is further divided based on a strict character limit to prevent memory overflows during inference.


\section{Structured Requirements Extraction}
The core component of the system is the extraction of structured data. To address the non-deterministic nature of LLMs, the \texttt{outlines} library is employed to constrain the generation process using Finite State Machine (FSM) logic.

\subsection{JSON Schema Definition}
A strict Pydantic model defines the output schema. This ensures that every processed job description yields a consistent data structure, regardless of the underlying LLM used. The schema is defined as follows:
\begin{center}
	\begin{lstlisting}[language=python, caption={JSON Schema for Requirements Extraction}, label={lst:requirements_schema}]

from typing import List
from pydantic import BaseModel, Field

class Requirements(BaseModel):
    """Normalized requirements schema for job extraction outputs."""
    skills: List[str] = Field(
        default_factory=list,
        description="List of required skills"
    )
    experiences: List[str] = Field(
        default_factory=list,
        description="List of experience requirements"
    )
    qualifications: List[str] = Field(
        default_factory=list,
        description="List of qualifications/certifications"
    )
    \end{lstlisting}
\end{center}

\subsection{Prompt Engineering}
The prompt provided to the LLM consists of a system instruction defining the persona and the task, followed by the specific job description chunk. The system prompt used is:
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{images/prompt}
	\caption{System prompt used for requirements extraction}
	\label{fig:prompt}
\end{figure}


\subsection{Inference Engine}
The inference logic is encapsulated within a generic \texttt{LLMExtractor} class, designed to be model-agnostic. This allows for seamless switching between different open-source models (e.g., Llama-3, Mistral, Qwen) by simply updating a configuration dictionary.

The extractor initializes the model and tokenizer, and wraps them with the \texttt{outlines.Generator}. The generation process is constrained to strictly follow the \texttt{Requirements} schema defined above.
\begin{center}
	\begin{lstlisting}[language=python, caption={LLM Extractor Implementation}, label={code:api-implementation}]
class LLMExtractor:
    def __init__(self, model_id, chunk_size, device_kwargs=None):
        self.model_id = model_id
        self.chunk_size = chunk_size
        self.device_kwargs = device_kwargs or {}
        self.generator = None
        self._load_model()

    def _load_model(self):
        # ... Memory management and cleanup ...
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            **self.device_kwargs,
            trust_remote_code=True,
        )
        # Initialize Outlines generator with schema constraint
        outlines_model = from_transformers(self.model, self.tokenizer)
        self.generator = Generator(outlines_model, Requirements)
    \end{lstlisting}
\end{center}
so it makes adding a new model or removing the ones exisiting easier by add 5 lines as dict to code:
\begin{center}
	\begin{lstlisting}[language=json, caption={llm-defintion}, label={lst:llm-defintion}]
{
  "qwen3-8b": {
    "model_id": "Qwen/Qwen3-8B",
    "chunk_size": 12000,
    "device_kwargs": {
      "device_map": "auto",
      "dtype": "torch.bfloat16"
    }
  }
}
    \end{lstlisting}
	\label{code:llm-defintion}
\end{center}

\subsection{Exception handling}
making sure that LLM output matches the json schema \ref{lst:requirements_schema}. although this is a good features but does not work always, as it can happen that the model does return an output that is not a valid json or missing a field, in that case it returns a Requirement Model where the fields have empty array.
\subsection{Result Aggregation}
Since job descriptions are processed in chunks, a single job posting may yield multiple \texttt{Requirements} objects. An aggregation step is implemented to merge these partial results. A \texttt{defaultdict(set)} is utilized to collect skills, experience, and qualifications across all chunks. The use of sets automatically handles deduplication, ensuring that if a skill is mentioned in multiple sections, it appears only once in the final output.
\subsection{G-Eval Metrics}
To integrate the evaluation strategy described in \autoref{conception:g-eval}, this work implements four metrics defined in \autoref{metrics}. Each metric specifies a natural-language description of the goal, a sequence of evaluation steps, the relevant test case fields (job description and/or extracted requirements), and a fixed success threshold of 0.7.

The evaluation pipeline first loads all JSON outputs from the requirements extraction stage and converts them into \texttt{LLMTestCase} objects, where the original job description serves as the input and the serialized \texttt{Requirements} object is used as \texttt{actual\_output}. Using DeepEval's \texttt{evaluate} function together with an external judge model, these metrics are executed asynchronously over all test cases, and the resulting scores, rationales are collected. A separate utility then aggregates the raw results by computing summary statistics per metric (average, minimum, maximum, and number of passes) and stores both per-file results and the aggregated overview in a JSON report. The example below \autoref{code:geval-metrics} shows the implementation of a G-Eval metric.

\begin{center}
    \begin{lstlisting}[language=python, caption={Implementation of a G-Eval metric}, label={code:geval-metrics}]
from deepeval.metrics import GEval

def create_evaluation_metrics(model: DeepEvalBaseLLM) -> List[GEval]:
    correctness_metric = GEval(
        name="Correctness",
        criteria=(
            "Evaluate whether the extracted job requirements are accurate and "
            "actually present in the input text."
        ),
        evaluation_steps=[
            "Read the input job description carefully",
            "Compare each extracted requirement with the input text",
            # further evaluation steps ...
        ],
        evaluation_params=[
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
        ],
        model=model,
        threshold=0.7,
    )
    # additional metrics: Completeness, Precision, Structure Quality
    return [correctness_metric, ...]
    \end{lstlisting}
\end{center}

\section{Embedding and Similarity Search}

\subsection{Vector Embedding}
To validate the similarity scores,
\subsection{Maxsim}
TODO: code of  Maxsim part