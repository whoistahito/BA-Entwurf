\chapter{Implementation}
\label{ch:implementation}

This chapter details the technical implementation of the concept presented in the previous chapter. It begins by describing the development environment. Following this, \Autoref{sec:impl-data-processing} explains the data preprocessing pipeline. \Autoref{sec:impl-lre} presents the implementation of LRE and the evaluation framework using LLM-as-a-judge. Finally, \Autoref{sec:impl-tss} details the implementation of TSS.

\section{Development Environment} \label{sec:impl-dev-env}
The system was implemented using the Python programming language (Version 3.10), chosen for its extensive ecosystem of data science and machine learning libraries. The core dependencies include:
\begin{itemize}
	\item \textbf{Markdownify}: For converting raw HTML content into structured Markdown text.
	\item \textbf{Outlines}: To enforce structured JSON / Pydantic Model generation from the LLMs, ensuring deterministic output formats.
	\item \textbf{FastAPI}: For serving the inference engine as a scalable web service.
	\item \textbf{Sentence-Transformers}: For generating vector embeddings of requirements and user profile.
	\item \textbf{DeepEval}: For the automated LLM-as-a-judge evaluation.
\end{itemize}
All experiments and inference tasks were conducted on a dual GPU setup (2x 12GB NVIDIA RTX 3080), ensuring consistent performance benchmarks across all tested models. \Autoref{fig:system_architecture} illustrates the high-level architecture of the implemented system.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/job-matching-system-architecture}
	\caption{System Architecture}
	\label{fig:system_architecture}
\end{figure}


\section{Data Preprocessing} \label{sec:impl-data-processing}
As outlined in the conception, the raw data consists of HTML documents. The preprocessing pipeline transforms this unstructured input into clean, chunked text suitable for LLM processing.

\subsection{HTML to Markdown Conversion}
The markdownify library is configured to strip unnecessary tags (such as scripts, styles, and images) while preserving the structural hierarchy of the document. Headings, lists, and paragraphs are retained, as these structural elements provide critical semantic cues for the extraction model. This reduction significantly lowers the token count while maintaining the semantic structure of the job posting.

\subsection{Hybrid Chunking Strategy} \label{subsec:impl-chunking}
To process job descriptions that exceed the context window of smaller LLMs, a hybrid chunking strategy is implemented. The text is primarily split based on Markdown headers (using the regex \verb|r'(#{1,6}\s+.*?\n)'|). This ensures that logical sections, such as Responsibilities or Requirements, remain intact within a single context window.

A fallback mechanism is implemented for cases where a section exceeds the maximum token limit. If a semantic chunk exceeds the configured size (defaulting to approximately 12,000 tokens, estimated via character count), it is further divided based on a strict character limit to prevent memory overflows during inference.


\section{Requirements Extraction} \label{sec:impl-lre}
The core component of the system is the extraction of structured data. To address the non-deterministic nature of LLMs, the outlines library is employed to constrain the generation process using FSM logic.

\subsection{Output Structure Definition}
A strict Pydantic model defines the output schema. This ensures that every processed job description yields a consistent data structure, regardless of the underlying LLM used. The schema is defined as follows:
\begin{center}
	\begin{lstlisting}[language=python, caption={Pydantic Schema for Requirements Extraction}, label={lst:requirements_schema}]

from typing import List
from pydantic import BaseModel, Field

class Requirements(BaseModel):
    """Normalized requirements schema for job extraction outputs."""
    skills: List[str] = Field(
        default_factory=list,
        description="List of required skills"
    )
    experiences: List[str] = Field(
        default_factory=list,
        description="List of experience requirements"
    )
    qualifications: List[str] = Field(
        default_factory=list,
        description="List of qualifications/certifications"
    )
    \end{lstlisting}
\end{center}

\subsection{Prompt Engineering}
The prompt provided to the LLM consists of a system instruction defining the persona and the task, followed by the specific job description chunk. For effective job matching, the system focuses exclusively on must-have requirements, as these are the essential prerequisites a candidate needs to apply for the job. \Autoref{fig:prompt} shows the system prompt, which requires the LLM to strictly adhere to this constraint, be concise, and only respond in the defined categories.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{images/prompt}
	\caption{System prompt used for requirements extraction}
	\label{fig:prompt}
\end{figure}


\subsection{Inference}
The inference logic is encapsulated within a generic \texttt{LLMExtractor} class, designed to be model-agnostic. This allows for seamless switching between different open-source models by simply updating a configuration dictionary.

The extractor initializes the model and tokenizer and wraps them with the \texttt{Generator} class from the outlines library. Passing the \texttt{Requirements} object defined in \Autoref{lst:requirements_schema} instructs the \texttt{Generator} to produce only outputs that match that schema. The implementation of this logic is shown in \Autoref{code:api-implementation}.

\begin{center}
	\begin{lstlisting}[language=python, caption={LLM Extractor Implementation}, label={code:api-implementation},float=h]
class LLMExtractor:
    def __init__(self, model_id, chunk_size, device_kwargs=None):
        self.model_id = model_id
        self.chunk_size = chunk_size
        self.device_kwargs = device_kwargs or {}
        self.generator = None
        self._load_model()

    def _load_model(self):
        # ... Memory management and cleanup ...
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            **self.device_kwargs,
            trust_remote_code=True,
        )
        # Initialize Outlines generator with schema constraint
        outlines_model = from_transformers(self.model, self.tokenizer)
        self.generator = Generator(outlines_model, Requirements)
    \end{lstlisting}
\end{center}

This design simplifies the process of managing models. Adding a new model or removing an existing one requires modifying the JSON entries. For example, \Autoref{lst:llm-defintion} shows how to add the Qwen3-8B model.

\begin{center}
	\begin{lstlisting}[language=json, caption={LLM Definition}, label={lst:llm-defintion},float=h]
{
  "qwen3-8b": {
    "model_id": "Qwen/Qwen3-8B",
    "chunk_size": 12000,
    "device_kwargs": {
      "device_map": "auto",
      "dtype": "torch.bfloat16"
    }
  }
}
    \end{lstlisting}
\end{center}

\subsection{Exception handling} \label{subsec:exception_handling}

After the output is generated, the output structure is validated against the Pydantic model using the built-in \texttt{validate\_json()} method.
 While the outlines library is designed to ensure that the model output follows the schema shown in \Autoref{lst:requirements_schema}, issues can still occur during generation. For example, if the model reaches its \texttt{max\_new\_tokens} limit, it may produce incomplete or invalid JSON that does not pass validation.
When this happens, the system handles the exception and returns a \texttt{Requirements} object in which the fields are empty lists.
This approach allows the rest of the pipeline to run without failing, but it also means that there is no clear indication that the output was invalid, which makes troubleshooting more challenging.

\subsection{Result Aggregation}
Since job descriptions are processed in chunks, a single job posting may yield multiple \texttt{Requirements} objects. An aggregation step is implemented to merge these partial results. A \texttt{defaultdict} of sets is utilized to collect skills, experience, and qualifications across all chunks. The use of sets automatically handles deduplication, ensuring that if a skill is mentioned in multiple sections, it appears only once in the final output.
\subsection{G-Eval Metrics}
To integrate the evaluation strategy described in \Autoref{conception:g-eval}, this work implements four metrics defined in \Autoref{metrics}. Each metric specifies a natural-language description of the goal, a sequence of evaluation steps, the relevant test case fields (job description and/or extracted requirements), and a fixed success threshold of 0.7. \Autoref{code:geval-metrics} shows an example metric definition.

\begin{center}
	\begin{lstlisting}[language=python, caption={Implementation of a G-Eval metric}, label={code:geval-metrics}]
from deepeval.metrics import GEval

def create_evaluation_metrics(model: DeepEvalBaseLLM) -> List[GEval]:
    correctness_metric = GEval(
        name="Correctness",
        criteria=(
        "Evaluate whether the extracted job requirements are"
        "accurate and actually present in the input text."
        ),
        evaluation_steps=[
            "Read the input job description carefully",
            "Compare each extracted requirement with the input text",
            # further evaluation steps ...
        ],
        evaluation_params=[
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
        ],
        model=model,
        threshold=0.7,
    )
    # additional metrics: Completeness, Alignment , Readability
    return [correctness_metric, ...]
    \end{lstlisting}
\end{center}

The evaluation pipeline first loads all JSON outputs from the requirements extraction stage and converts them into \texttt{LLMTestCase} objects, where the original job description serves as the input and the serialized \texttt{Requirements} object is used as \texttt{actual\_output}. Finally, the system utilizes DeepEval's \texttt{evaluate} function and a judge model with external api to execute these metrics asynchronously over all test cases, collecting the resulting scores.

\section{Embedding and Similarity Search} \label{sec:impl-tss}
This section describes the implementation of the similarity matching component introduced in the conception. The goal is to compute a semantic similarity score between extracted job requirements and a user profile.

\subsection{Vector Embedding}
The embedding functionality is implemented using the \texttt{sentence-transformers} library, which provides pre-trained transformer models optimized for semantic similarity tasks \cite{sentence-bert}. The model all-MiniLM-L6-v2 is loaded once as a singleton to avoid repeated initialization overhead during inference.

\subsection{MaxSim}
The MaxSim algorithm described in \Autoref{subsec:maxsim} is implemented in the \texttt{compute\_maxsim} function. The implementation directly follows the conceptual formulation: for each job requirement, find the maximum similarity to any user profile item, then average these maximum scores.
\begin{center}
    \begin{lstlisting}[language=python, caption={Implementation of MaxSim Logic}, label={code:compute_maxsim}]
def compute_maxsim(user_items: List[str],
                   job_items: List[str],
                   model: SentenceTransformer) -> float:
    if not user_items or not job_items:
        return 0.0

    # 1. Encoding
    user_embeddings = model.encode(user_items, convert_to_tensor=True)
    job_embeddings = model.encode(job_items, convert_to_tensor=True)

    # 2. Similarity Matrix
    cosine_scores = util.cos_sim(job_embeddings, user_embeddings)

    # 3. Max-Over-Rows (dim=1)
    max_scores, _ = torch.max(cosine_scores, dim=1)

    # 4. Aggregation
    return max_scores.mean().item()
    \end{lstlisting}
\end{center}
The \texttt{util.cos\_sim} function from \texttt{sentence-transformers} computes the full similarity matrix $C$ between all job and user embeddings. The \texttt{torch.max} operation along dimension 1 extracts the maximum similarity for each job requirement (row), and \texttt{mean()} computes the final field score as defined in \Autoref{eq:maxsim_coverage}.

\subsection{Weighted Aggregation}
\Autoref{lst:weighted-agg} shows how final similarity score is combined to the MaxSim scores from all three fields using configurable weights. By default, skills receive a weight of 0.5, experiences 0.3, and qualifications 0.2, reflecting the prioritization of skills-based matching discussed in \Autoref{sub:conception-weighted-field}.

\begin{center}
	\begin{lstlisting}[language=python, caption={Weighted score aggregation}, label={lst:weighted-agg}]
def compute_similarity(user_profile, extracted_requirements,
                       weights=None) -> SimilarityScore:
    if weights is None:
        weights = {"skills": 0.5, "experiences": 0.3,
                   "qualifications": 0.2}

    model = get_model()
    overall_scores = {}

    for field in ['skills', 'experiences', 'qualifications']:
        user_items = getattr(user_profile, field, [])
        job_items = getattr(extracted_requirements, field, [])
        overall_scores[field] = compute_maxsim(user_items,
                                                job_items, model)

    weighted_score = sum(
        overall_scores[field] * weights.get(field, 0.0)
        for field in overall_scores
    )
    return SimilarityScore(score=weighted_score)
    \end{lstlisting}
\end{center}

This design allows the weights to be adjusted via the API without modifying the codebase, enabling experimentation with different prioritization strategies.
