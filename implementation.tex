\chapter{Implementation}
\label{ch:implementation}

This chapter details the technical realization of the concept presented in the previous chapter. It describes the development environment, the data processing pipeline, the implementation of the requirements extraction using LLMs, the evaluation methodology, and the realization of the similarity search algorithms.

\section{Development Environment}
The system was implemented using the Python programming language (Version 3.10), chosen for its extensive ecosystem of data science and machine learning libraries. The core dependencies include:
\begin{itemize}
	\item \textbf{Markdownify}: For converting raw HTML content into structured Markdown text.
	\item \textbf{Outlines}: To enforce structured JSON / Pydantic Model generation from the LLMs, ensuring deterministic output formats.
	\item \textbf{FastAPI}: For serving the inference engine as a scalable web service.
	\item \textbf{Sentence-Transformers}: For generating vector embeddings of requirements and user profile.
	\item \textbf{DeepEval}: For the automated ``LLM-as-a-judge'' evaluation pipeline.
\end{itemize}
All experiments and inference tasks were conducted on a dual GPU setup (2x 12GB NVIDIA RTX 3080), ensuring consistent performance benchmarks across all tested models. \autoref{fig:system_architecture} illustrates the high-level architecture of the implemented system.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/job-matching-system-architecture}
	\caption{System Architecture}
	\label{fig:system_architecture}
\end{figure}


\section{Data Preprocessing}
As outlined in the conception, the raw data consists of HTML documents. The preprocessing pipeline transforms this unstructured input into clean, chunked text suitable for LLM processing.

\subsection{HTML to Markdown Conversion}
The \texttt{markdownify} library is configured to strip unnecessary tags (such as scripts, styles, and images) while preserving the structural hierarchy of the document. Headings, lists, and paragraphs are retained, as these structural elements provide critical semantic cues for the extraction model. This reduction significantly lowers the token count while maintaining the semantic structure of the job posting.

\subsection{Hybrid Chunking Strategy}
To process job descriptions that exceed the context window of smaller LLMs, a hybrid chunking strategy is implemented. The text is primarily split based on Markdown headers (using the regex \verb|r'(#{1,6}\s+.*?\n)'|). This ensures that logical sections—such as 'Responsibilities' or 'Requirements'—remain intact within a single context window.

A fallback mechanism is implemented for cases where a section exceeds the maximum token limit. If a semantic chunk exceeds the configured size (defaulting to approximately 12,000 tokens, estimated via character count), it is further divided based on a strict character limit to prevent memory overflows during inference.


\section{Structured Requirements Extraction}
The core component of the system is the extraction of structured data. To address the non-deterministic nature of LLMs, the \texttt{outlines} library is employed to constrain the generation process using Finite State Machine (FSM) logic.

\subsection{Output Structure Definition}
A strict Pydantic model defines the output schema. This ensures that every processed job description yields a consistent data structure, regardless of the underlying LLM used. The schema is defined as follows:
\begin{center}
	\begin{lstlisting}[language=python, caption={Pydantic Schema for Requirements Extraction}, label={lst:requirements_schema}]

from typing import List
from pydantic import BaseModel, Field

class Requirements(BaseModel):
    """Normalized requirements schema for job extraction outputs."""
    skills: List[str] = Field(
        default_factory=list,
        description="List of required skills"
    )
    experiences: List[str] = Field(
        default_factory=list,
        description="List of experience requirements"
    )
    qualifications: List[str] = Field(
        default_factory=list,
        description="List of qualifications/certifications"
    )
    \end{lstlisting}
\end{center}

\subsection{Prompt Engineering}
The prompt provided to the LLM consists of a system instruction defining the persona and the task, followed by the specific job description chunk. The system prompt used is:
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{images/prompt}
	\caption{System prompt used for requirements extraction}
	\label{fig:prompt}
\end{figure}


\subsection{Inference Engine}
The inference logic is encapsulated within a generic \texttt{LLMExtractor} class, designed to be model-agnostic. This allows for seamless switching between different open-source models (e.g., Llama, Mistral, Qwen) by simply updating a configuration dictionary.

The extractor initializes the model and tokenizer, and wraps them with the \texttt{outlines.Generator}. The generation process is constrained to strictly follow the \texttt{Requirements} schema defined above.
\begin{center}
	\begin{lstlisting}[language=python, caption={LLM Extractor Implementation}, label={code:api-implementation}]
class LLMExtractor:
    def __init__(self, model_id, chunk_size, device_kwargs=None):
        self.model_id = model_id
        self.chunk_size = chunk_size
        self.device_kwargs = device_kwargs or {}
        self.generator = None
        self._load_model()

    def _load_model(self):
        # ... Memory management and cleanup ...
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            **self.device_kwargs,
            trust_remote_code=True,
        )
        # Initialize Outlines generator with schema constraint
        outlines_model = from_transformers(self.model, self.tokenizer)
        self.generator = Generator(outlines_model, Requirements)
    \end{lstlisting}
\end{center}
so it makes adding a new model or removing the ones exisiting easier by add 5 lines as dict to code:
\begin{center}
	\begin{lstlisting}[language=json, caption={LLM Definition}, label={lst:llm-defintion},float=h]
{
  "qwen3-8b": {
    "model_id": "Qwen/Qwen3-8B",
    "chunk_size": 12000,
    "device_kwargs": {
      "device_map": "auto",
      "dtype": "torch.bfloat16"
    }
  }
}
    \end{lstlisting}
	\label{code:llm-defintion}
\end{center}

\subsection{Exception handling} \label{subsec:exception_handling}

After the output is generated, the output structure is validated against the Pydantic model using the built-in \texttt{validate\_json()} method.
 While the outlines library is designed to ensure that the model output follows the schema shown in \autoref{lst:requirements_schema}, issues can still occur during generation. For example, if the model reaches its \texttt{max\_new\_tokens} limit, it may produce incomplete or invalid JSON that does not pass validation.
When this happens, the system handles the exception and returns a Requirements object in which the fields are empty lists.
This approach allows the rest of the pipeline to run without failing, but it also means that there is no clear indication that the output was invalid, which makes troubleshooting more challenging.

\subsection{Result Aggregation}
Since job descriptions are processed in chunks, a single job posting may yield multiple \texttt{Requirements} objects. An aggregation step is implemented to merge these partial results. A \texttt{defaultdict(set)} is utilized to collect skills, experience, and qualifications across all chunks. The use of sets automatically handles deduplication, ensuring that if a skill is mentioned in multiple sections, it appears only once in the final output.
\subsection{G-Eval Metrics}
To integrate the evaluation strategy described in \autoref{conception:g-eval}, this work implements four metrics defined in \autoref{metrics}. Each metric specifies a natural-language description of the goal, a sequence of evaluation steps, the relevant test case fields (job description and/or extracted requirements), and a fixed success threshold of 0.7. \autoref{code:geval-metrics} shows an example metric definition.

\begin{center}
	\begin{lstlisting}[language=python, caption={Implementation of a G-Eval metric}, label={code:geval-metrics}]
from deepeval.metrics import GEval

def create_evaluation_metrics(model: DeepEvalBaseLLM) -> List[GEval]:
    correctness_metric = GEval(
        name="Correctness",
        criteria=(
        "Evaluate whether the extracted job requirements are"
        "accurate and actually present in the input text."
        ),
        evaluation_steps=[
            "Read the input job description carefully",
            "Compare each extracted requirement with the input text",
            # further evaluation steps ...
        ],
        evaluation_params=[
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
        ],
        model=model,
        threshold=0.7,
    )
    # additional metrics: Completeness, Alignment , Readability
    return [correctness_metric, ...]
    \end{lstlisting}
\end{center}

The evaluation pipeline first loads all JSON outputs from the requirements extraction stage and converts them into \texttt{LLMTestCase} objects, where the original job description serves as the input and the serialized \texttt{Requirements} object is used as \texttt{actual\_output}. Using DeepEval's \texttt{evaluate} function together with an external judge model, these metrics are executed asynchronously over all test cases, and the resulting scores, rationales are collected.
\section{Embedding and Similarity Search}
This section describes the implementation of the similarity matching component introduced in the conception. The goal is to compute a semantic similarity score between extracted job requirements and a user profile.

\subsection{Vector Embedding}
The embedding functionality is implemented using the \texttt{sentence-transformers} library, which provides pre-trained transformer models optimized for semantic similarity tasks.\cite{sentence-bert} The model \textit{all-MiniLM-L6-v2} is loaded once as a singleton to avoid repeated initialization overhead during inference.

\subsection{MaxSim}
The MaxSim algorithm described in \autoref{subsec:maxsim} is implemented in the \texttt{compute\_maxsim} function. The implementation directly follows the conceptual formulation: for each job requirement, find the maximum similarity to any user profile item, then average these maximum scores.
\begin{center}
    \begin{lstlisting}[language=python, caption={Implementation of MaxSim Logic}, label={code:compute_maxsim}]
def compute_maxsim(user_items: List[str],
                   job_items: List[str],
                   model: SentenceTransformer) -> float:
    if not user_items or not job_items:
        return 0.0

    # 1. Encoding
    user_embeddings = model.encode(user_items, convert_to_tensor=True)
    job_embeddings = model.encode(job_items, convert_to_tensor=True)

    # 2. Similarity Matrix
    cosine_scores = util.cos_sim(job_embeddings, user_embeddings)

    # 3. Max-Over-Rows (dim=1)
    max_scores, _ = torch.max(cosine_scores, dim=1)

    # 4. Aggregation
    return max_scores.mean().item()
    \end{lstlisting}
\end{center}
The \texttt{util.cos\_sim} function from \texttt{sentence-transformers} computes the full similarity matrix $C$ between all job and user embeddings. The \texttt{torch.max} operation along dimension 1 extracts the maximum similarity for each job requirement (row), and \texttt{mean()} computes the final field score as defined in \autoref{eq:maxsim_coverage}.

\subsection{Weighted Aggregation}
The final similarity score combines the MaxSim scores from all three fields using configurable weights. By default, skills receive a weight of 0.5, experiences 0.3, and qualifications 0.2, reflecting the prioritization of skills-based matching discussed in the conception.

\begin{center}
	\begin{lstlisting}[language=python, caption={Weighted score aggregation}, label={lst:weighted-agg}]
def compute_similarity(user_profile, extracted_requirements,
                       weights=None) -> SimilarityScore:
    if weights is None:
        weights = {"skills": 0.5, "experiences": 0.3,
                   "qualifications": 0.2}

    model = get_model()
    overall_scores = {}

    for field in ['skills', 'experiences', 'qualifications']:
        user_items = getattr(user_profile, field, [])
        job_items = getattr(extracted_requirements, field, [])
        overall_scores[field] = compute_maxsim(user_items,
                                                job_items, model)

    weighted_score = sum(
        overall_scores[field] * weights.get(field, 0.0)
        for field in overall_scores
    )
    return SimilarityScore(score=weighted_score)
    \end{lstlisting}
\end{center}

This design allows the weights to be adjusted via the API without modifying the codebase, enabling experimentation with different prioritization strategies.
